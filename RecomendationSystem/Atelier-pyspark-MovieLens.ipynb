{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies des grosses données](https://github.com/wikistat/Ateliers-Big-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommandation de Films par Filtrage Collaboratif: [NMF](http://wikistat.fr/pdf/st-m-explo-nmf.pdf) de la librairie [SparkML](https://spark.apache.org/docs/latest/ml-guide.html) de <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Ce calepin traite d'un problème classique de recommandation par filtrage collaboratif en utilisant les ressources de la librairie [MLlib de Spark]([http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS) avec l'API pyspark. Le problème général est décrit en [introduction](https://github.com/wikistat/Ateliers-Big-Data/tree/master/3-MovieLens) et dans une [vignette](http://wikistat.fr/pdf/st-m-datSc3-colFil.pdf) de [Wikistat](http://wikistat.fr/). Il est appliqué aux données publiques du site [GroupLens](http://grouplens.org/datasets/movielens/). L'objectif est de tester les méthodes et la procédure d'optimisation sur le plus petit jeu de données composé de 100k notes  de 943 clients sur 1682 films où chaque client a au moins noté 20 films. Les jeux de données plus gros (1M, 10M, 20M notes) peuvent être utilisés pour \"passer à l'échelle volume\". \n",
    "\n",
    "Ce calepin s'inspire des exemples de la [documentation](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS) et d'un [tutoriel](https://github.com/jadianes/spark-movie-lens/blob/master/notebooks/building-recommender.ipynb) de [Jose A. Dianes](https://www.codementor.io/jadianes). Le sujet a été traité lors d'un [Spark Summit](https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html).\n",
    "\n",
    "L'objectif est d'utiliser ces seules données pour proposer des recommandations.  Les données initiales sont sous la forme d'une matrice **très creuse** (*sparse*) contenant des notes ou évaluations. **Attention**, les \"0\" de la matrice ne sont pas des notes mais des *données manquantes*, le film n'a pas encore été vu ou évalué. \n",
    "\n",
    "Un algorithme satisfaisant à l'objectif de *complétion de grande matrice creuse*, et implémenté dans un logiciel libre d'accès est disponible dans la librairie [softImpute de R](https://cran.r-project.org/web/packages/softImpute/index.html). SOn utilisaiton est décrite dans un autre [calepin](https://github.com/wikistat/Ateliers-Big-Data/blob/master/3-MovieLens/Atelier-MovieLens-softImpute.ipynb). La version de [NMF](http://wikistat.fr/pdf/st-m-explo-nmf.pdf) de [MLlib de Spark](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS) autorise permet également la complétion.\n",
    "\n",
    "En revanche,la version  de NMF incluse dans la librairie [Scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) traite également des [matrices creuses](http://docs.scipy.org/doc/scipy/reference/sparse.html) mais le critère (moindres carrés) optimisé considère les \"0\" comme des notes nulles, pas comme des données manquantes. *Elle n'est pas adaptée au problème de complétion*, contrairement à celle de MLliB. Il faudrait sans doute utiliser la librairie [nonnegfac](https://github.com/kimjingu/nonnegfac-python) en Python  de [Kim et al. (2014)](http://link.springer.com/content/pdf/10.1007%2Fs10898-013-0035-4.pdf); **à tester**!\n",
    "\n",
    "Dans la première partie, le plus petit fichier est partagé en trois échantillons: apprentissage, validation et test; l'optimisation du rang de la factorisation (nombre de facteurs latents) est réalisée par minimisation de l'erreur estimée sur l'échantillon de validation.\n",
    "\n",
    "Ensuite le plus gros fichier est utilisé pour évaluer l'impact de la taille de la base d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Importation des données en HDFS\n",
    "Les données doivent être stockées à un emplacement accessibles de tous les noeuds du cluster pour permettre la construction de la base de données réparties (RDD). Dans une utilisation monoposte (*standalone*) de *Spark*, elles sont simplement chargées dans le répertoire courant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des fichiers si ce n'est déjà fait\n",
    "#Renseignez ici le dossier où vous souhaitez stocker le fichier téléchargé.\n",
    "DATA_PATH=\"\" \n",
    "import urllib.request\n",
    "# fichier réduit\n",
    "f = urllib.request.urlretrieve(\"http://www.math.univ-toulouse.fr/~besse/Wikistat/data/ml-ratings100k.csv\",DATA_PATH+\"ml-ratings100k.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont lues comme une seule ligne de texte avant d'être restructurées au bon format d'une *matrice creuse* à savoir une liste de triplets contenant les  indices de ligne, de colonne et la note pour les seules valeurs renseignées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les données au format texte dans un RDD\n",
    "\n",
    "small_ratings_raw_data = sc.textFile(DATA_PATH+\"ml-ratings100k.csv\")\n",
    "\n",
    "# Identifier et afficher la première ligne\n",
    "small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0]\n",
    "print(small_ratings_raw_data_header)\n",
    "\n",
    "# Create RDD without header\n",
    "all_lines = small_ratings_raw_data.filter(lambda l : l!=small_ratings_raw_data_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les champs (user, item, note) dans un nouveau RDD\n",
    "from pyspark.sql import Row\n",
    "split_lines = all_lines.map(lambda l : l.split(\",\"))\n",
    "ratingsRDD = split_lines.map(lambda p: Row(user=int(p[0]), item=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
    "\n",
    "# .cache() : le RDD est conservé en mémoire une fois traité\n",
    "ratingsRDD.cache()\n",
    "\n",
    "# Display the two first rows\n",
    "ratingsRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert RDD to DataFrame\n",
    "ratingsDF = spark.createDataFrame(ratingsRDD)\n",
    "ratingsDF.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimisation du rang sur l'échantillon 10k\n",
    "Le fichier comporte 10 000 évaluations croisant les avis de mille utilisateurs sur les films qu'ils ont vus parmi 1700."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Constitution des échantillons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Séparation aléatoire en trois échantillons apprentissage, validation et test. Le paramètre de rang est optimisé en minimisant l'estimaiton de l'erreur sur l'échantillon test. Cette stratégie, plutôt qu'ue validation croisée est plus adaptée à des données massives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tauxTrain=0.6\n",
    "tauxVal=0.2\n",
    "tauxTes=0.2\n",
    "# Si le total est inférieur à 1, les données sont sous-échantillonnées.\n",
    "(trainDF, validDF, testDF) = ratingsDF.randomSplit([tauxTrain, tauxVal, tauxTes])\n",
    "# validation et test à prédire, sans les notes\n",
    "validDF_P = validDF.select(\"user\", \"item\")\n",
    "testDF_P = testDF.select(\"user\", \"item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.take(2), validDF_P.take(2), testDF_P.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Optimisation du rang de la NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'erreur d'imputation des données, donc de recommandation, est estimée sur l'échantillon de validation pour différentes valeurs (grille) du rang de la factorisation matricielle. \n",
    "\n",
    "Il faudrait en principe aussi optimiser la valeur du paramètre de pénalisation pris à 0.1 par défaut.\n",
    "\n",
    "*Point important:* l'erreur d'ajustement de la factorisation ne prend en compte que les valeurs listées dans la matrice creuses, pas les \"0\" qui sont des données manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "import math\n",
    "import collections\n",
    "# Initialisation du générateur\n",
    "seed = 5\n",
    "# Nombre max d'itérations (ALS)\n",
    "maxIter = 10\n",
    "# Régularisation L1; à optimiser également\n",
    "regularization_parameter = 0.1\n",
    "# Choix d'une grille pour les valeurs du rang à optimiser\n",
    "ranks = [4, 8, 12]\n",
    "\n",
    "#Initialisation variable \n",
    "# création d'un dictionaire pour stocker l'erreur par rang testé\n",
    "errors = collections.defaultdict(float)\n",
    "tolerance = 0.02\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "for rank in ranks:\n",
    "    als = ALS( rank=rank, seed=seed, maxIter=maxIter,\n",
    "                      regParam=regularization_parameter)\n",
    "    model = als.fit(trainDF)\n",
    "    # Prévision de l'échantillon de validation\n",
    "    predDF = model.transform(validDF).select(\"prediction\",\"rating\")\n",
    "    #Remove unpredicter row due to no-presence of user in the train dataset\n",
    "    pred_without_naDF = predDF.na.drop()\n",
    "    # Calcul du RMSE\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "    rmse = evaluator.evaluate(pred_without_naDF)\n",
    "    print(\"Root-mean-square error for rank %d = \"%rank + str(rmse))\n",
    "    errors[rank] = rmse\n",
    "    if rmse < min_error:\n",
    "        min_error = rmse\n",
    "        best_rank = rank\n",
    "# Meilleure solution\n",
    "print('Rang optimal: %s' % best_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Résultats et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelques prévisions\n",
    "pred_without_naDF.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prévision finale de l'échantillon test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On concatane la DataFrame Train et Validatin\n",
    "trainValidDF = trainDF.union(validDF)\n",
    "# On crée un model avec le nouveau Dataframe complété d'apprentissage et le rank fixé à la valeur optimal \n",
    "als = ALS( rank=best_rank, seed=seed, maxIter=maxIter,\n",
    "                  regParam=regularization_parameter)\n",
    "model = als.fit(trainValidDF)\n",
    "#Prediction sur la DataFrame Test\n",
    "testDF = model.transform(testDF).select(\"prediction\",\"rating\")\n",
    "#Remove unpredicter row due to no-presence of user in the trai dataset\n",
    "pred_without_naDF = predDF.na.drop()\n",
    "# Calcul du RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                            predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(pred_without_naDF)\n",
    "print(\"Root-mean-square error for rank %d = \"%best_rank + str(rmse))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Analyse du fichier complet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MovieLens propose un plus gros fichier avec 20M de notes (138000 utilisateurs, 27000 films). Ce fichier est utilisé pour extraire un fichier test de deux millions de notes à reconstruire. Les paramètres précédemment optimisés, ils pourraient sans doute l'être mieux, sont appliqués pour une succesion d'estimation / prévision avec une taille croissante de l'échantillon d'apprentissage. Il aurait été plus élégant d'automatiser le travail dans une boucle mais lorsque les données sont les plus volumineuses des comportement mal contrôlés de Spark peuvent provoquer des plantages par défaut de mémoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Lecture des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier est prétraité de manière analogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des fichiers si ce n'est déjà fait\n",
    "import urllib.request\n",
    "# fichier complet mais compressé\n",
    "f = urllib.request.urlretrieve(\"http://www.math.univ-toulouse.fr/~besse/Wikistat/data/ml-ratings20M.zip\",DATA_PATH+\"ml-ratings20M.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip downloaded file\n",
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile(DATA_PATH+\"ml-ratings20M.zip\", 'r')\n",
    "zip_ref.extractall(DATA_PATH)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les données au format texte dans un RDD\n",
    "ratings_raw_data = sc.textFile(DATA_PATH+\"ratings20M.csv\")\n",
    "# Identifier et afficher la première ligne\n",
    "ratings_raw_data_header = ratings_raw_data.take(1)[0]\n",
    "ratings_raw_data_header\n",
    "\n",
    "# Create RDD without header\n",
    "all_lines = ratings_raw_data.filter(lambda l : l!=ratings_raw_data_header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les champs (user, item, note) dans un nouveau RDD\n",
    "split_lines = all_lines.map(lambda l : l.split(\",\"))\n",
    "ratingsRDD = split_lines.map(lambda p: Row(user=int(p[0]), item=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
    "\n",
    "# Display the two first rows\n",
    "ratingsRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert RDD to DataFrame\n",
    "ratingsDF = spark.createDataFrame(ratingsRDD)\n",
    "ratingsDF.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Echantillonnage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction de l'échantillon test et éventuellement sous-échantillonnage de l'échantillon d'apprentissage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tauxTest=0.1\n",
    "# Si le total est inférieur à 1, les données sont sous-échantillonnées.\n",
    "(trainTotDF,  testDF) = ratingsDF.randomSplit([1-tauxTest, tauxTest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sous-échantillonnage de l'apprentissage permettant de \n",
    "# tester pour des tailles croissantes de cet échantillon\n",
    "tauxEch=0.2\n",
    "(trainDF, DropData) = trainTotDF.randomSplit([tauxEch, 1-tauxEch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF.take(2), trainDF.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Estimation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle est estimé en utilisant les valeurs des paramètres obtenues dans l'étape précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time_start=time.time()\n",
    "# Initialisation du générateur\n",
    "seed = 5\n",
    "# Nombre max d'itérations (ALS)\n",
    "maxIter = 10\n",
    "# Régularisation L1 (valeur par défaut)\n",
    "regularization_parameter = 0.1\n",
    "best_rank = 8\n",
    "# Estimation pour chaque valeur de rang\n",
    "als = ALS(rank=rank, seed=seed, maxIter=maxIter,\n",
    "                      regParam=regularization_parameter)\n",
    "model = als.fit(trainDF)\n",
    "time_end=time.time()\n",
    "time_als=(time_end - time_start)\n",
    "print(\"ALS prend %d s\" %(time_als)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Prévision de l'échantillon test et erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prévision de l'échantillon de validation\n",
    "predDF = model.transform(testDF).select(\"prediction\",\"rating\")\n",
    "#Remove unpredicter row due to no-presence of user in the train dataset\n",
    "pred_without_naDF = predDF.na.drop()\n",
    "# Calcul du RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                            predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(pred_without_naDF)\n",
    "print(\"Root-mean-square error for rank %d = \"%best_rank + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques résultats montrant l'évolution du temps de calcul et de l'erreur de prévision en fonction de la taille de l'échantillon d'apprentissage. Attention, il est probable que la valeur des paramètres optimaux dépendent de la taille de l'échantillon d'apprentissage.\n",
    "\n",
    "Taille | Temps(s) | RMSE\n",
    "-------|-------|------\n",
    "217439 | 70    | 1.65\n",
    "1029416| 73    | 1.06\n",
    "2059855| 72    | 1.05\n",
    "4119486| 89    | 0.88\n",
    "6176085| 99    | 0.85\n",
    "10301909| 117  | 0.83\n",
    "12361034| 125  | 0.83\n",
    "14414907| 137  | 0.82\n",
    "16474087| 148  | 0.818\n",
    "18538142| 190  | 0.816\n",
    "20596263| 166  | 0.82\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PySpark (Spark 2.3.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
