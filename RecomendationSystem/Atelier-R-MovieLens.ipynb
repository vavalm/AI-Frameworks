{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies des grosses data](https://github.com/wikistat/Ateliers-Big-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommandation par Filtrage Collaboratif: avec R\n",
    "### Résumé\n",
    "Présentation sommaire des systèmes de recommandation. Exemple jouet de filtrage collaboratif traité avec R par [décomposition en valeurs singulières](http://wikistat.fr/pdf/st-m-explo-alglin.pdf), factorisation non négative de matrice ou [NMF](http://wikistat.fr/pdf/st-m-explo-nmf.pdf) et complétion de matrice; exemple réaliste de recommandation de films ([MovieLens](http://grouplens.org/datasets/movielens/)). Les mêmes données sont traitées de façon plus performante dans un [autre calepin](https://github.com/wikistat/Ateliers-Big-Data/blob/master/3-MovieLens/Atelier-MovieLens-pyspark.ipynb) utilisant Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: [filtrage collaboratif](http://wikistat.fr/pdf/st-m-datSc3-colFil.pdf)\n",
    "### 1.1 Objectif\n",
    "Tous les sites marchands mettent en place des systèmes de recommandation pour déterminer les produits les plus susceptibles d'intéresser les internautes / clients en visite. Lorsque ceux-ci sont basées sur les seules informations concernant les interactions clients *x* produits, ils sont nommés *filtrage collaboratif*. Parmi ces derniers, les systèmes les plus aboutis sont basés sur la recherche d'un modèle de quelques *facteurs latents* susceptibles d'expliquer en faible dimension les interactions entre clients et produits.  \n",
    "\n",
    "D'autres systèmes sont basés sur des connaissances clients (*user-based*) ou sur des connaissances produits (*item-based*)ou encore sur des approches mixtes. Ils ne sont pas abordés ici.\n",
    "\n",
    "Les données se mettent sous la forme d'une matrice **X**, toujours très creuse, contenant pour chaque client *i* (ligne) le *nombre d'achats* du produit *j* (colonne) ou une *note d'appréciation* de 1 à 5 lorsqu'il s'agit de films (Netflix),  musiques (itune), livres... \n",
    "\n",
    "**Attention**, la valeur \"*0*\" a du sens lorsqu'il s'agit d'un *nombre d'achats* alors qu'elle doit signifier une *donnée manquante* dans le cas d'une notation. Dans le premier cas, (*valeur 0*) le filtrage collaboratif est obtenu par une *factorisation de matrice*. Dans le 2ème (*0 données manquante*), il s'agit d'une *complétion*.\n",
    "\n",
    "### 1.2 Factorisation\n",
    "La [décomposition en valeurs singulières](http://wikistat.fr/pdf/st-m-explo-alglin.pdf) (SVD) d'une matrice ainsi que la  [*Non Negativ Matrix Factorization*](http://wikistat.fr/pdf/st-m-explo-nmf.pdf) (NMF) sont utilisées dans ce contexte pour rechercher les facteurs (matrices **W** et **H**) reconstruisant au mieux la matrice **X** *#* **W.H** avec une contrainte de *parcimonie* ou faible rang sur les matrices **W** et **H**.  Contrairement à la SVD où les facteurs sont recherchés orthogonaux 2 à 2, la NMF impose la contrainte de non négativité des matrices pour construire les facteurs de la décomposition. Ces facteurs ne permettent plus de représentation comme en [ACP](http://wikistat.fr/pdf/st-m-explo-acp.pdf) ou en [MDS](http://wikistat.fr/pdf/st-m-explo-mds.pdf) mais au moins une classification non supervisée tant des objets lignes que des  objets colonnes de la matrice initiale. Ces classifications sont respectivement basées sur les matrices **W** et **H** des facteurs dits latents. \n",
    "\n",
    "Schématiquement, $w_{ij}$ dénote l'appétence du *i*-ème utilisateur pour le *j*-ème facteur latent, tandis que $h_{jk}$ décrit quelle part du *k*-ième item intervient dans le *j*-ème facteur latent; le modèle suppose que la note $x_{ik}$ est la somme, sur tous les facteurs latents *j*, des produits $w_{ij}\\times h_{jk}$. \n",
    "\n",
    "La SVD, est basée sur un critère de moindre carrés [norme trace des matrices](http://wikistat.fr/pdf/st-m-explo-alglin.pdf). La librairie `NMF` (Gaujoux et Seoighe, 2010)\\ de R propose plusieurs algorithmes de factorisation non négative, principalement:  *Multiplicative update algorithms* et *Alternate least Square (ALS)*, adaptés à deux fonctions possibles de perte: divergence de Kullback-Leibler (KL) ou moindres carrés (norme trace). \n",
    "\n",
    "**Attention**, les choix d'option: fonction objectif, algorithme, rang des matrices, influencent fortement les résultats obtenus et ce d'autant plus que les algorithmes (NMF) convergent (au mieux) vers des optimums locaux. La SVD bénéficie d'une convergence \"globale\" mais est moins adaptée au contexte car les solutions sont moins cohérentes avec l'objectif recherché: des notes ou comptages nécessairement positifs.\n",
    "\n",
    "### 1.3 Complétion\n",
    "Lorsque, les données sont des notes d'appréciation, la valeur \"0\" signifie en principe une valeur manquante. L'usage de la NMF ou de la SVD est alors abusif. Cette situation a été largement popularisée avec le concours [Netflix](http://www.netflixprize.com/) à 1M$. Il a été abordé de façon théorique par Candes et Tao (2010) comme un problème de *complétion de matrice* sous contrainte de parcimonie;  problème difficile, dont de très nombreuses approximations et implémentations ont depuis été proposées. Une simple utilisation est proposée ici dont l'algorithme (Mazumder et al. 2010) implémenté dans R (`softImpute`) conduit également à une factorisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exemple jouet\n",
    "### 2.1 Les données \n",
    "Des données fictives triviales sont testées afin d'illustrer la démarche. Elles contiennent des nombres d'achats de certains produits ou des notes d'appréciation et peuvent être complétées à loisir au gré de votre imagination.\n",
    "\n",
    "Le fichier fictif `recom-jouet.dat` est dans le même dépôt que ce tutoriel. Il contient un nombre d'achats par client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jouet=read.table(\"recom-jouet.dat\")\n",
    "jouet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=6, repr.plot.height=3)\n",
    "boxplot(jouet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont bien creuses mais les variables s'expriment dans des unités et donc avec des variances très différentes. Une forme de normalisation peut s'avérer  nécessaire. Elle concerne à la fois les produits (colonnes ou variables), car certains (chocolat) sont plébiscités plus que d'autres, ainsi que les clients qui peuvent avoir des échelles de notation ou des profiles de consommation très différents . \n",
    "\n",
    "### 2.2 Recommandation par NMF\n",
    "Chargement de la librairie et identification des algorithmes disponibles. Plusieurs initialisation sont possibles; seule celle aléatoire par défaut est utilisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(NMF)\n",
    "nmfAlgorithm()\n",
    "nmfAlgorithm(\"brunet\")\n",
    "nmfAlgorithm(\"lee\")\n",
    "nmfAlgorithm(\"snmf/l\")\n",
    "nmfAlgorithm(\"snmf/r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Identifier la fonction perte; les deux derniers algorithmes sont issus de l'ALS. \n",
    "\n",
    "\n",
    "Comparer les méthodes en exécutant pour chacune d'entre elles 10 factorisations de rang 5. Les exécutions sont répétées car la convergence locale dépend de l'initialisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.multi.method=nmf(jouet, 5,nrun=10, list(\"brunet\",\"lee\",\"snmf/l\",\"snmf/r\"), \n",
    "                     seed = 111, .options =\"t\")\n",
    "compare(res.multi.method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=6, repr.plot.height=5)\n",
    "consensusmap(res.multi.method,hclustfun=\"ward\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Choisir la méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estim.r=nmf(jouet,2:6,method=\"snmf/l\", nrun=10,seed=111)\n",
    "plot(estim.r)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=5)\n",
    "consensusmap(estim.r) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Choix du rang des matrices de la décomposition?\n",
    "\n",
    "Une fois méthode et rang déterminés, itérer plusieurs fois l'exécution pour retenir la \"meilleure\" puis extraction des \"facteurs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf.jouet=nmf(jouet,4,method=\"snmf/l\",nrun=30,seed=111)\n",
    "w=basis(nmf.jouet)\n",
    "h=coef(nmf.jouet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Production de classifications non-supervisées et graphiques associés aux matrices `w` et `h` de la factorisation.  Ceci permet d'identifier des groupes de clients au regard de leur consommation ou préférences comme de construire des classes de produits appréciés simultanément."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    "basismap(nmf.jouet,hclustfun=\"ward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=5, repr.plot.height=3)\n",
    "coefmap(nmf.jouet,hclustfun=\"ward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme c'est logique, le dendrogramme produit dans les cartes précédentes est directement issu des classifications ascendantes hiérarchiques calculées à partir des distances euclidiennes entre les lignes de `w` d'une part et les colonnes de `h` d'autre part.\n",
    "\n",
    "La classification des objets est représentable dans les coordonnées d'un MDS ou dans les composantes d'une ACP des \"facteurs\" de la NMF; c'est équivalent en considérant la distance euclidienne définie à partir de ces facteurs.\n",
    "\n",
    "Les produits à plus forte occurrence ou note peuvent prendre trop d'importance, les facteurs sont réduits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distmod.h=dist(scale(t(h)), method=\"eucl\")\n",
    "mdjouet= cmdscale(distmod.h, k=2)\n",
    "hclusmod.h=hclust(distmod.h,method=\"ward.D\")\n",
    "options(repr.plot.width=5, repr.plot.height=4)\n",
    "plot(hclusmod.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dN.h=dimnames(h)[[2]]\n",
    "hclasmod.h = cutree(hclusmod.h,k=4)\n",
    "plot(mdjouet, type=\"n\", xlab=\"\", ylab=\"\",main=\"\")\n",
    "text(mdjouet,dN.h,col=hclasmod.h)\n",
    "abline(v=0,h=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distmod.v=dist(scale(w), method=\"eucl\")\n",
    "mdjouet= cmdscale(distmod.v, k=2)\n",
    "hclusmod.v=hclust(distmod.v,method=\"ward.D\")\n",
    "plot(hclusmod.v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hclasmod.v = cutree(hclusmod.v,k=2)\n",
    "dN.v=dimnames(w)[[1]]\n",
    "plot(mdjouet, type=\"n\", xlab=\"\", ylab=\"\",main=\"\")\n",
    "text(mdjouet,dN.v,col=hclasmod.v)\n",
    "abline(v=0,h=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il n'est pas possible comme en ACP ou AFCM de mettre en relation les deux représentations des lignes et colonnes, individus et variables de la matrice factorisée. Cela peut être fait de façon détournée à l'aide d'une *heatmap* qui  intègre les classifications obtenues en réordonnant les lignes et colonnes de **X**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intégration des deux classifications\n",
    "aheatmap(jouet,Rowv=hclusmod.v, Colv=hclusmod.h,annRow=as.factor(hclasmod.v),\n",
    "         annCol=as.factor(hclasmod.h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommandation\n",
    "L'objectif de rechercher les produits les plus susceptibles d'intéresser les clients. Celui-ci est atteint en reconstruisant une approximation de la matrice `x` par produit des matrices des facteurs. \n",
    "\n",
    "*Les couples (client, produit) pour lesquels les valeurs reconstruites sont le plus élevées alors qu'il n'y a pas eu d'achat ou de notation, sont ceux qui sont ciblés afin de proposer le produit identifié au client de ce couple.*\n",
    "\n",
    "**Attention**, le choix du rang est déterminant. En utilisant le choix optimal précédent (*r=4*) la reconstruction est finalement \"trop\" bonne et aucune recommandation n'émerge de la reconstruction de `x`. Le choix *r=2*, sous-optimal, fait ressortir des couples candidats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécution avec r=2\n",
    "nmf.jouet=nmf(jouet,2,method=\"snmf/l\", nrun=30,seed=111)\n",
    "# Matrice reconstruite\n",
    "xchap=w%*%h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparer avec les données initiales, identifier le plus fort score reconstruit par client et identifier le produit correspondant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod=apply(xchap-10*jouet,1,function(x) which.max(x))\n",
    "cbind(dN.v,dN.h[prod])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remarques*\n",
    "- La démarche s'applique également à de simples matrices de $(0,1)$ de présence / absence d'achat.\n",
    "- Les matrices peuvent être très grandes (données massives) sur des sites marchand, il est alors nécessaire d'utiliser des librairies avec représentation adaptée de matrices creuses pour réduire l'occupation mémoire. Seules les valeurs non nulles sont stockées.\n",
    "- L'initialisation, ou *cold start*, de la matrice est un problème bien identifié. Cela concerne l'introduction d'un nouveau client ou d'un nouveau produit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Par SVD\n",
    "La décomposition en valeurs singulières propose également une factorisation de la matrice **X**=**UL V'**. Celle-ci a de bien meilleure propriétés numériques dont l'unicité de la solution optimale qui est atteinte pour un rang fixé. Des contraintes de parcimonie ou de régularité peuvent également être associées à la fonction perte quadratique (*sparse SVD*). \n",
    "\n",
    "#### ACP\n",
    "La démarche est équivalente et découle directement de la SVD donc de l'[analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf) de **X**. Remarquer une forte similitude entre les représentions obtenus par MDS des facteurs de le NMF et celles de l'ACP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biplot(prcomp(jouet,scale=TRUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommandation\n",
    "Approximation de rang 2 par SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=svd(jouet)\n",
    "# Matrice reconstruite\n",
    "xchap=res$u[,1:2]%*%diag(res$d[1:2])%*%t(res$v[,1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparer avec les données initiales, identifier le plus fort score reconstruit par client, identifier le produit correspondant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod=apply(xchap-10*jouet,1,function(x) which.max(x))\n",
    "cbind(dN.v,dN.h[prod])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comparer aec les recommandations précédentes sur cet exemple trivial.\n",
    "\n",
    "## 2.4 Par complétion de matrice\n",
    "On considère que les données sont des notes d'appréciation: la valeur \"0\" signifie en principe une valeur manquante et le problème est formellement celui d'une complétion de matrice. La librairie `softImpute` de R en propose une solution par un algorithme de SVD seuillée (Hastie et al. 2010). Il s'agit donc d'approcher une matrice très creuse avec beaucoup de valeurs manquantes par une matrice de faible rang. L'algorithme est analogue à un algorithme EM pour imputation de données manquantes. La fonction R accepte la classe de représentation des grandes matrices creuses.\n",
    "\n",
    "Les valeurs nulles sont remplacées par des valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jouet.na=jouet\n",
    "jouet.na[jouet==0]=NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La recommandation se fait comme avec la SVD. Une étude plus approfondie de cet algorithme et de son usage s'avère nécessaire, notamment pour régler rang et paramètre de pénalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"softImpute\")\n",
    "library(softImpute)\n",
    "res=softImpute(jouet.na,rank.max=2,type=\"svd\",lambda=1)\n",
    "# Matrice reconstruite\n",
    "xchap=res$u%*%diag(res$d)%*%t(res$v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparer avec les données initiales, identifier le plus fort score reconstruit par client, identifier le produit correspondant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod=apply(xchap-10*jouet,1,function(x) which.max(x))\n",
    "cbind(dN.v,dN.h[prod])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comparer les résultats.\n",
    "\n",
    "Cette étude n'est qu'une brève introduction au problème du filtrage collaboratif. De nombreuses questions n'ont pas été abordées dont le *cold start* et surtout celle très importante de l'*évaluation* de tels systèmes. Un procédé simple consiste à volontairement supprimer des valeurs afin de voir si le système les retrouve et en quelle proportion; c'était le principe du concours Netflix retenue dans la suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recommandation de films\n",
    "### 3.1 Objectif\n",
    "Le filtrage collaboratif est appliqué aux données publiques du site [GroupLens](http://grouplens.org/datasets/movielens/). L'objectif est de tester les méthodes et la procédure d'optimisation sur le plus petit jeu de données composé de 100k notes  de 943 clients sur 1682 films où chaque client a au moins noté 20 films. Les jeux de données plus gros (1M, 10M, 20M notes) peuvent être utilisés pour \"passer à l'échelle volume\". \n",
    "\n",
    "Les données initiales sont sous la forme d'une matrice **très creuse** (*sparse*) contenant des notes ou évaluations. Les*0* de la matrice ne sont pas des notes mais des *données manquantes*, le film n'a pas encore été vu ou évalué. \n",
    "\n",
    "La complétion de grande matrice creuse est obtnue par l'algorithme implémenté dans la librairie [softImpute de R](https://cran.r-project.org/web/packages/softImpute/index.html). Un [autre calepin](https://github.com/wikistat/Ateliers-Big-Data/blob/master/3-MovieLens/Atelier-MovieLens-pyspark.ipynb) utilise la version de [NMF](http://wikistat.fr/pdf/st-m-explo-nmf.pdf) de [MLlib de Spark](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS) qui permet également la complétion.\n",
    "\n",
    "En revanche,la version  de NMF incluse dans la librairie [Scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) traite également des [matrices creuses](http://docs.scipy.org/doc/scipy/reference/sparse.html) mais le critère (moindres carrés) optimisé considère les \"0\" comme des notes nulles, pas comme des données manquantes.  \n",
    "\n",
    "Dans la première partie, le plus petit fichier est partagé en trois échantillons: apprentissage, validation et test; l'optimisation du rang de la factorisation (nombre de facteurs latents) est réalisée par minimisation de l'erreur estimée sur l'échantillon de validation.\n",
    "\n",
    "Ensuite le plus gros fichier est utilisé pour évaluer l'impact de la taille de la base d'apprentissage.\n",
    "\n",
    "Néanmoins, comme les résultats obtenus ne concurrencent pas ceux de MLlib et sont même assez *décevants* ([Besse et al. 2016](https://hal.archives-ouvertes.fr/hal-01350099)), ce calepin ne fait que décrire *a minima* le code de mise en oeuvre de la librairie sur les données MovieLens. \n",
    "\n",
    "### 3.2  Etude de la matrice 100k\n",
    "Les fichiers de données sont dans le même dépôt que ce calepin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(softImpute)\n",
    "# lecture\n",
    "dBrut=read.csv(\"ml-ratings100k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction d'un échantillon test et d'un échantillon d'apprentissage\n",
    "dTestInd=sample(nrow(dBrut),nrow(dBrut)/10,replace=FALSE)\n",
    "dTest=dBrut[dTestInd,1:3]\n",
    "dTrain=dBrut[-dTestInd,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mise au format d'une matrice creuse\n",
    "dTrainSparse=Incomplete(dTrain$userId,dTrain$movieId,dTrain$rating)\n",
    "# appel de la fonction\n",
    "res=softImpute(dTrainSparse,rank.max=4,type=\"als\",lambda=1,maxit=200)\n",
    "# complétion\n",
    "recom=impute(res,dTest[,1],dTest[,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de l'erreur (RMSE)\n",
    "sqrt(mean((dTest[,3]-recom)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Etude de la matrice complète\n",
    "*Attention*, le fichier des données est trop volumineux pour un dépôt public de Github. Il est disponible dans le répertoire [`data`](http://wikistat.fr/data) de [Wikistat](http://wikistat.fr/) et compressé: `ratings20M.csv`. Le charger et le décompresser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture de la matrice\n",
    "dBrut=read.csv(\"ratings20M.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction de l'échantillon test\n",
    "dTestInd=sample(nrow(dBrut),nrow(dBrut)/10,replace=FALSE)\n",
    "dTest=dBrut[dTestInd,1:3]\n",
    "nrow(dTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sous-échantillonage de l'échantillon d'apprentissage\n",
    "dInter=dBrut[-dTestInd,1:3]\n",
    "taux=0.1\n",
    "dTrainInd=sample(nrow(dInter),nrow(dInter)*taux,replace=FALSE)\n",
    "dTrain=dInter[dTrainInd,1:3]\n",
    "# Matrice d'échantillonnage sparse\n",
    "dTrainSparse=Incomplete(dTrain$userId,dTrain$movieId,dTrain$rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention**. La démarche adoptée ici pour trouver les meilleurs valeurs des paramètres (rang, pénalisation) est *abusive*. Les valeurs ont été optimisées sur l'échantillon test sans pour autant atteindre des résultats concurrentiels avec la [solution](https://github.com/wikistat/Ateliers-Big-Data/blob/master/3-MovieLens/Atelier-MovieLens-pyspark.ipynb) utilisant MLlib de Spark. Voir la comparaison détaillée par [Besse et al. 2016](https://hal.archives-ouvertes.fr/hal-01350099)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorisation\n",
    "t1=Sys.time()\n",
    "res=softImpute(dTrainSparse,rank.max=10,type=\"als\",lambda=20,maxit=200)\n",
    "t2=Sys.time()\n",
    "# Reconstruction\n",
    "recom=impute(res,dTest[,1],dTest[,2])\n",
    "#RMSE\n",
    "sqrt(mean((dTest[,3]-recom)**2))\n",
    "difftime(t2,t1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apprentissage avec fichier complet (taux=1): 20M de notes\n",
    "\n",
    "rang | lambda | temps (') | rmse \n",
    "----|---------|-------|-----\n",
    " 4  |  1       |  5.6 |  1.07   \n",
    " 10 |  10  |  12.6 |  1.02  \n",
    " 10 |  20  |  12.2 |  1.033 \n",
    " 15 |  10  |  19.4 |  1.016\n",
    " 20 |   1  |  26.9  | 1.02 \n",
    " 20 |  10  |  26.1  | 1.016 \n",
    " 20 |  15  |  24.4 |  1.018\n",
    " 20 |  20  |  27.0  | 1.016 \n",
    " 30 |  20  |  40.1 |  1.02\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
