{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies de l'intelligence Artificielle](https://github.com/wikistat/AI-Frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning \n",
    " \n",
    "Source : [https://github.com/ageron/handson-ml](https://github.com/ageron/handson-ml)\n",
    "\n",
    "\n",
    "https://github.com/breeko/Simple-Reinforcement-Learning-with-Tensorflow/blob/master/Part%202%20-%20Policy-based%20Agents%20with%20Keras.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Gym Librairie\n",
    "<a href=\"https://gym.openai.com/\" ><img src=\"https://gym.openai.com/assets/dist/home/header/home-icon-54c30e2345.svg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be using [OpenAI gym](https://gym.openai.com/), a great toolkit for developing and comparing Reinforcement Learning algorithms. It provides many environments for your learning *agents* to interact with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures and animations\n",
    "%matplotlib inline\n",
    "%matplotlib nbagg\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.initializers as ki\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.losses as klo\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "# Gym Librairy\n",
    "import gym\n",
    "\n",
    "def display_environment(env):\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    env.close()\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 0 0 0 0 0 0 0 0 0 0 1 2 1 2 1 2 1 3 \n",
      "States: 0 3 \n",
      "States: 0 0 0 0 0 1 2 1 2 1 3 \n",
      "States: 0 0 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 3 \n",
      "States: 0 0 0 0 1 3 \n",
      "States: 0 0 0 0 0 0 0 3 \n",
      "States: 0 0 0 0 0 0 0 0 0 0 3 \n",
      "States: 0 1 3 \n"
     ]
    }
   ],
   "source": [
    "transition_probabilities = [\n",
    "        [0.7, 0.2, 0.0, 0.1],  # from s0 to s0, s1, s2, s3\n",
    "        [0.0, 0.0, 0.9, 0.1],  # from s1 to ...\n",
    "        [0.0, 1.0, 0.0, 0.0],  # from s2 to ...\n",
    "        [0.0, 0.0, 0.0, 1.0],  # from s3 to ...\n",
    "    ]\n",
    "\n",
    "n_max_steps = 50\n",
    "\n",
    "def print_sequence(start_state=0):\n",
    "    current_state = start_state\n",
    "    print(\"States:\", end=\" \")\n",
    "    for step in range(n_max_steps):\n",
    "        print(current_state, end=\" \")\n",
    "        if current_state == 3:\n",
    "            break\n",
    "        current_state = np.random.choice(range(4), p=transition_probabilities[current_state])\n",
    "    else:\n",
    "        print(\"...\", end=\"\")\n",
    "    print()\n",
    "\n",
    "for _ in range(10):\n",
    "    print_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_fire\n",
      "States (+rewards): 0 (10) 0 1 (-50) 2 (40) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) ... Total rewards = 240\n",
      "States (+rewards): 0 (10) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) 0 (10) 0 1 (-50) ... Total rewards = 40\n",
      "States (+rewards): 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 1 (-50) ... Total rewards = 140\n",
      "States (+rewards): 0 (10) 0 (10) 0 (10) 0 1 (-50) 2 (40) 0 1 (-50) 2 (40) 0 ... Total rewards = 180\n",
      "States (+rewards): 0 (10) 0 (10) 0 (10) 0 1 (-50) 2 (40) 0 1 (-50) 2 (40) 0 (10) ... Total rewards = -50\n",
      "Summary: mean=128.1, std=134.344856, min=-340, max=510\n",
      "\n",
      "policy_random\n",
      "States (+rewards): 0 (10) 0 0 0 0 0 (10) 0 (10) 0 0 (10) 0 (10) ... Total rewards = 120\n",
      "States (+rewards): 0 0 0 0 0 0 (10) 0 (10) 0 0 0 ... Total rewards = 80\n",
      "States (+rewards): 0 0 (10) 0 0 1 1 (-50) 2 (40) 0 0 (10) 0 (10) ... Total rewards = 0\n",
      "States (+rewards): 0 (10) 0 (10) 0 1 1 (-50) 2 (40) 0 1 (-50) 2 (40) 0 ... Total rewards = 190\n",
      "States (+rewards): 0 1 1 1 1 1 1 (-50) 2 (40) 0 (10) 0 ... Total rewards = -10\n",
      "Summary: mean=-25.8, std=88.986109, min=-360, max=210\n",
      "\n",
      "policy_safe\n",
      "States (+rewards): 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 1 1 1 ... Total rewards = 60\n",
      "States (+rewards): 0 (10) 0 (10) 0 (10) 0 (10) 0 1 1 1 1 1 ... Total rewards = 40\n",
      "States (+rewards): 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 1 1 1 ... Total rewards = 60\n",
      "States (+rewards): 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 1 1 1 1 ... Total rewards = 50\n",
      "States (+rewards): 0 1 1 1 1 1 1 1 1 1 ... Total rewards = 0\n",
      "Summary: mean=23.2, std=27.992783, min=0, max=260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transition_probabilities = [\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]], # in s0, if action a0 then proba 0.7 to state s0 and 0.3 to state s1, etc.\n",
    "        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "        [None, [0.8, 0.1, 0.1], None],\n",
    "    ]\n",
    "\n",
    "rewards = [\n",
    "        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]],\n",
    "    ]\n",
    "\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]\n",
    "\n",
    "def policy_fire(state):\n",
    "    return [0, 2, 1][state]\n",
    "\n",
    "def policy_random(state):\n",
    "    return np.random.choice(possible_actions[state])\n",
    "\n",
    "def policy_safe(state):\n",
    "    return [0, 0, 1][state]\n",
    "\n",
    "class MDPEnvironment(object):\n",
    "    def __init__(self, start_state=0):\n",
    "        self.start_state=start_state\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.total_rewards = 0\n",
    "        self.state = self.start_state\n",
    "    def step(self, action):\n",
    "        next_state = np.random.choice(range(3), p=transition_probabilities[self.state][action])\n",
    "        reward = rewards[self.state][action][next_state]\n",
    "        self.state = next_state\n",
    "        self.total_rewards += reward\n",
    "        return self.state, reward\n",
    "\n",
    "def run_episode(policy, n_steps, start_state=0, display=True):\n",
    "    env = MDPEnvironment()\n",
    "    if display:\n",
    "        print(\"States (+rewards):\", end=\" \")\n",
    "    for step in range(n_steps):\n",
    "        if display:\n",
    "            if step == 10:\n",
    "                print(\"...\", end=\" \")\n",
    "            elif step < 10:\n",
    "                print(env.state, end=\" \")\n",
    "        action = policy(env.state)\n",
    "        state, reward = env.step(action)\n",
    "        if display and step < 10:\n",
    "            if reward:\n",
    "                print(\"({})\".format(reward), end=\" \")\n",
    "    if display:\n",
    "        print(\"Total rewards =\", env.total_rewards)\n",
    "    return env.total_rewards\n",
    "\n",
    "for policy in (policy_fire, policy_random, policy_safe):\n",
    "    all_totals = []\n",
    "    print(policy.__name__)\n",
    "    for episode in range(1000):\n",
    "        all_totals.append(run_episode(policy, n_steps=100, display=(episode<5)))\n",
    "    print(\"Summary: mean={:.1f}, std={:1f}, min={}, max={}\".format(np.mean(all_totals), np.std(all_totals), np.min(all_totals), np.max(all_totals)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning works by watching an agent play (e.g., randomly) and gradually improving its estimates of the Q-Values. Once it has accurate Q-Value estimates (or close enough), then the optimal policy consists in choosing the action that has the highest Q-Value (i.e., the greedy policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 3\n",
    "n_actions = 3\n",
    "n_steps = 20\n",
    "alpha = 0.01\n",
    "gamma = 0.99\n",
    "exploration_policy = policy_random\n",
    "q_values = np.full((n_states, n_actions), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    q_values[state][actions]=0\n",
    "#display(q_values)\n",
    "env = MDPEnvironment()\n",
    "for step in range(n_steps):\n",
    "    #print(step)\n",
    "    \n",
    "    action = exploration_policy(env.state) \n",
    "    state = env.state\n",
    "    next_state, reward = env.step(action)\n",
    "    next_value = np.max(q_values[next_state]) # greedy policy\n",
    "    #print(\"Current state : %s, Action choosen : %s, lead to state : %s, reward obtain : %s next value: %s\"%(action, state, next_state, reward, next_value))\n",
    "    q_values[state, action] = (1-alpha)*q_values[state, action] + alpha*(reward + gamma * next_value)\n",
    "    #display(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09801  ,  0.       ,  0.       ],\n",
       "       [ 0.       ,       -inf, -0.99104  ],\n",
       "       [      -inf,  0.7969801,       -inf]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state):\n",
    "    return np.argmax(q_values[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09801  ,  0.       ,  0.       ],\n",
       "       [ 0.       ,       -inf, -0.99104  ],\n",
       "       [      -inf,  0.7969801,       -inf]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States (+rewards): 0 (10) 0 1 1 1 1 1 1 1 1 ... Total rewards = 10\n",
      "States (+rewards): 0 (10) 0 1 1 1 1 1 1 1 1 ... Total rewards = 10\n",
      "States (+rewards): 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) ... Total rewards = 110\n",
      "States (+rewards): 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 1 1 1 ... Total rewards = 60\n",
      "States (+rewards): 0 (10) 0 1 1 1 1 1 1 1 1 ... Total rewards = 10\n",
      "Summary: mean=22.4, std=26.780170, min=0, max=220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_totals = []\n",
    "for episode in range(1000):\n",
    "    all_totals.append(run_episode(optimal_policy, n_steps=100, display=(episode<5)))\n",
    "print(\"Summary: mean={:.1f}, std={:1f}, min={}, max={}\".format(np.mean(all_totals), np.std(all_totals), np.min(all_totals), np.max(all_totals)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Play MsPacman Using the DQN Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: Unfortunately, the first version of the book contained two important errors in this section.\n",
    "\n",
    "1. The actor DQN and critic DQN should have been named _online DQN_ and _target DQN_ respectively. Actor-critic algorithms are a distinct class of algorithms.\n",
    "2. The online DQN is the one that learns and is copied to the target DQN at regular intervals. The target DQN's only role is to estimate the next state's Q-Values for each possible action. This is needed to compute the target Q-Values for training the online DQN, as shown in this equation:\n",
    "\n",
    "$y(s,a) = \\text{r} + \\gamma . \\underset{a'}{\\max} \\, Q_\\text{target}(s', a')$\n",
    "\n",
    "* $y(s,a)$ is the target Q-Value to train the online DQN for the state-action pair $(s, a)$.\n",
    "* $r$ is the reward actually collected after playing action $a$ in state $s$.\n",
    "* $\\gamma$ is the discount rate.\n",
    "* $s'$ is the state actually reached after played action $a$ in state $s$.\n",
    "* $a'$ is one of the possible actions in state $s'$.\n",
    "* $Q_\\text{target}(s', a')$ is the target DQN's estimate of the Q-Value of playing action $a'$ while in state $s'$.\n",
    "\n",
    "I hope these errors did not affect you, and if they did, I sincerely apologize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the MsPacman environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MsPacman-v0\")\n",
    "obs = env.reset()\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the images is optional but greatly speeds up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 80, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mspacman_color = 210 + 164 + 74\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    img = obs[1:176:2, ::2] # crop and downsize\n",
    "    img = img.sum(axis=2) # to greyscale\n",
    "    img[img==mspacman_color] = 0 # Improve contrast\n",
    "    img = (img // 3 - 128).astype(np.int8) # normalize from -128 to 127\n",
    "    return img.reshape(88, 80, 1)/128\n",
    "\n",
    "img = preprocess_observation(obs)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the `preprocess_observation()` function is slightly different from the one in the book: instead of representing pixels as 64-bit floats from -1.0 to 1.0, it represents them as signed bytes (from -128 to 127). The benefit is that the replay memory will take up roughly 8 times less RAM (about 6.5 GB instead of 52 GB). The reduced precision has no visible impact on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(11, 7))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Original observation (160×210 RGB)\")\n",
    "plt.imshow(obs)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.title(\"Preprocessed observation (88×80 greyscale)\")\n",
    "plt.imshow(img.reshape(88, 80), interpolation=\"nearest\", cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: instead of using `tf.contrib.layers.convolution2d()` or `tf.contrib.layers.conv2d()` (as in the first version of the book), we now use the `tf.layers.conv2d()`, which did not exist when this chapter was written. This is preferable because anything in contrib may change or be deleted without notice, while `tf.layers` is part of the official API. As you will see, the code is mostly the same, except that the parameter names have changed slightly:\n",
    "* the `num_outputs` parameter was renamed to `filters`,\n",
    "* the `stride` parameter was renamed to `strides`,\n",
    "* the `_fn` suffix was removed from parameter names that had it (e.g., `activation_fn` was renamed to `activation`),\n",
    "* the `weights_initializer` parameter was renamed to `kernel_initializer`,\n",
    "* the weights variable was renamed to `\"kernel\"` (instead of `\"weights\"`), and the biases variable was renamed from `\"biases\"` to `\"bias\"`,\n",
    "* and the default `activation` is now `None` instead of `tf.nn.relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 88\n",
    "input_width = 80\n",
    "input_channels = 1\n",
    "initializer =  ki.VarianceScaling()\n",
    "\n",
    "\n",
    "\n",
    "def q_network():\n",
    "    q_model = km.Sequential()\n",
    "    q_model.add(kl.Conv2D(32,kernel_size=(8,8),strides=(4,4),padding=\"SAME\", activation=\"relu\", kernel_initializer = initializer, \n",
    "                          input_shape=(input_height, input_width, input_channels), data_format=\"channels_last\"))\n",
    "    q_model.add(kl.Conv2D(64,kernel_size=(4,4),strides=(2,2),padding=\"SAME\", activation=\"relu\",  kernel_initializer = initializer))\n",
    "    q_model.add(kl.Conv2D(64,kernel_size=(3,4),strides=(1,1),padding=\"SAME\", activation=\"relu\",  kernel_initializer = initializer))\n",
    "    q_model.add(kl.Flatten())\n",
    "    q_model.add(kl.Dense(512, activation=\"relu\", kernel_initializer=initializer))\n",
    "    q_model.add(kl.Dense(9, activation=\"linear\", kernel_initializer=initializer))\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.95\n",
    "    Opt = ko.SGD(lr=learning_rate, momentum=momentum , decay=0.0, nesterov=True)\n",
    "    q_model.compile(loss='mean_squared_error',optimizer=Opt) # later change to square loss if error >1 else linear\n",
    "\n",
    "\n",
    "\n",
    "    return q_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if False : \n",
    "    with tf.variable_scope(\"train\"):\n",
    "        X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "        y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        q_value = tf.reduce_sum(online_q_values * tf.one_hot(X_action, n_outputs),\n",
    "                                axis=1, keepdims=True)\n",
    "        error = tf.abs(y - q_value)\n",
    "        clipped_error = tf.clip_by_value(error, 0.0, 1.0)\n",
    "        linear_error = 2 * (error - clipped_error)\n",
    "        loss = tf.reduce_mean(tf.square(clipped_error) + linear_error)\n",
    "\n",
    "        global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "        training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in the first version of the book, the loss function was simply the squared error between the target Q-Values (`y`) and the estimated Q-Values (`q_value`). However, because the experiences are very noisy, it is better to use a quadratic loss only for small errors (below 1.0) and a linear loss (twice the absolute error) for larger errors, which is what the code above computes. This way large errors don't push the model parameters around as much. Note that we also tweaked some hyperparameters (using a smaller learning rate, and using Nesterov Accelerated Gradients rather than Adam optimization, since adaptive gradient algorithms may sometimes be bad, according to this [paper](https://arxiv.org/abs/1705.08292)). We also tweaked a few other hyperparameters below (a larger replay memory, longer decay for the $\\epsilon$-greedy policy, larger discount rate, less frequent copies of the online DQN to the target DQN, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this `ReplayMemory` class instead of a `deque` because it is much faster for random access (thanks to @NileshPS who contributed it). Moreover, we default to sampling with replacement, which is much faster than sampling without replacement for large replay memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self,buffer_size=500000):\n",
    "        \"\"\" Data structure used to hold game experiences \"\"\"\n",
    "        # Buffer will contain [state,action,reward,next_state,done]\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        \"\"\" Adds list of experiences to the buffer \"\"\"\n",
    "        # Extend the stored experiences\n",
    "        self.buffer.extend(experience)\n",
    "        # Keep the last buffer_size number of experiences\n",
    "        self.buffer = self.buffer[-self.buffer_size:]\n",
    "        \n",
    "    def sample(self, size):\n",
    "        \"\"\" Returns a sample of experiences from the buffer \"\"\"\n",
    "        sample_idxs = np.random.randint(len(self.buffer),size=size)\n",
    "        sample_output = [self.buffer[idx] for idx in sample_idxs]\n",
    "        sample_output = np.reshape(sample_output,(size,-1))\n",
    "        return sample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_memories(batch_size):\n",
    "    cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "    for memory in replay_memory.sample(batch_size):\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_min = 0.1\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 2000000\n",
    "\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs) # random action\n",
    "    else:\n",
    "        return np.argmax(q_values) # optimal action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_random_start = 1\n",
    "prob_random_end = 0.1\n",
    "annealing_steps = 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_graph(main_graph, target_graph, tau):\n",
    "    updated_weights = (np.array(main_graph.get_weights()) * tau) + \\\n",
    "        (np.array(target_graph.get_weights()) * (1 - tau))\n",
    "    target_graph.set_weights(updated_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 4000000  # total number of training steps\n",
    "pre_train_episodes = 1000  # start training after 10,000 game iterations\n",
    "update_freq = 4  # run a training step every 4 game iterations\n",
    "save_steps = 1000  # save the model every 1,000 training steps\n",
    "copy_steps = 10000  # copy online DQN to target DQN every 10,000 training steps\n",
    "y = 0.99\n",
    "skip_start = 90  # Skip the start of every game (it's just waiting time).\n",
    "batch_size = 50\n",
    "iteration = 0  # game iterations\n",
    "num_epochs=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few variables for tracking progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val = np.infty\n",
    "game_length = 0\n",
    "total_max_q = 0\n",
    "mean_max_q = 0.0\n",
    "tau=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num episode: 100 Mean reward: 213.4000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 200 Mean reward: 201.0000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 300 Mean reward: 203.0000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 400 Mean reward: 209.2000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 500 Mean reward: 225.3000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 600 Mean reward: 216.3000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 700 Mean reward: 205.4000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 800 Mean reward: 194.8000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 900 Mean reward: 200.0000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 1000 Mean reward: 203.1000 Prob random: 1.0000, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Reset everything\n",
    "K.clear_session()\n",
    "\n",
    "# Setup our Q-networks\n",
    "online_q_values = q_network()\n",
    "target_q_values = q_network()\n",
    "\n",
    "\n",
    "# Make the networks equal\n",
    "update_target_graph(online_q_values, target_q_values, 1)\n",
    "\n",
    "# Setup our experience replay\n",
    "experience_replay = ExperienceReplay()\n",
    "\n",
    "# We'll begin by acting complete randomly. As we gain experience and improve,\n",
    "# we will begin reducing the probability of acting randomly, and instead\n",
    "# take the actions that our Q network suggests\n",
    "prob_random = prob_random_start\n",
    "prob_random_drop = (prob_random_start - prob_random_end) / annealing_steps\n",
    "\n",
    "num_steps = [] # Tracks number of steps per episode\n",
    "rewards = [] # Tracks rewards per episode\n",
    "total_steps = 0 # Tracks cumulative steps taken in training\n",
    "\n",
    "print_every = 100 # How often to print status\n",
    "save_every = 5 # How often to save\n",
    "\n",
    "losses = [0] # Tracking training losses\n",
    "\n",
    "num_episode = 0\n",
    "\n",
    "while num_episode < num_episodes:\n",
    "    # Create an experience replay for the current episode\n",
    "    episode_buffer = ExperienceReplay()\n",
    "    \n",
    "     # Get the game state from the environment\n",
    "        \n",
    "        \n",
    "    state = env.reset()\n",
    "    for skip in range(skip_start): # skip the start of each game\n",
    "        state, reward, done, info = env.step(0)\n",
    "    state = preprocess_observation(state)\n",
    "    \n",
    "    done = False # Game is complete\n",
    "    sum_rewards = 0 # Running sum of rewards in episode\n",
    "    cur_step = 0 # Running sum of number of steps taken in episode\n",
    "    \n",
    "    \n",
    "    while not(done):\n",
    "        cur_step += 1\n",
    "        total_steps += 1\n",
    "\n",
    "        if np.random.rand() < prob_random or \\\n",
    "            num_episode < pre_train_episodes:\n",
    "                # Act randomly based on prob_random or if we\n",
    "                # have not accumulated enough pre_train episodes\n",
    "                action = np.random.randint(9)\n",
    "        else:\n",
    "            # Decide what action to take from the Q network\n",
    "            action = np.argmax(online_q_values.predict(np.array([state])))\n",
    "\n",
    "        # Take the action and retrieve the next state, reward and done\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_observation(next_state)\n",
    "\n",
    "        # Setup the episode to be stored in the episode buffer\n",
    "        episode = np.array([[state],action,reward,[next_state],done])\n",
    "        episode = episode.reshape(1,-1)\n",
    "\n",
    "        # Store the experience in the episode buffer\n",
    "        episode_buffer.add(episode)\n",
    "\n",
    "        # Update the running rewards\n",
    "        sum_rewards += reward\n",
    "\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "    if num_episode > pre_train_episodes:\n",
    "        # Training the network\n",
    "        if prob_random > prob_random_end:\n",
    "                # Drop the probability of a random action\n",
    "                prob_random -= prob_random_drop\n",
    "                \n",
    "        if num_episode % update_freq == 0:\n",
    "            for num_epoch in range(num_epochs):\n",
    "                # Train batch is [[state,action,reward,next_state,done],...]\n",
    "                train_batch = experience_replay.sample(batch_size)\n",
    "\n",
    "                # Separate the batch into its components\n",
    "                train_state, train_action, train_reward, \\\n",
    "                    train_next_state, train_done = train_batch.T\n",
    "\n",
    "                # Convert the action array into an array of ints so they can be used for indexing\n",
    "                train_action = train_action.astype(np.int)\n",
    "\n",
    "                # Stack the train_state and train_next_state for learning\n",
    "                train_state = np.vstack(train_state)\n",
    "                train_next_state = np.vstack(train_next_state)\n",
    "\n",
    "                # Our predictions (actions to take) from the main Q network\n",
    "                target_q = target_q_values.predict(train_state)\n",
    "\n",
    "                # The Q values from our target network from the next state\n",
    "                target_q_next_state = online_q_values.predict(train_next_state)\n",
    "                train_next_state_action = np.argmax(target_q_next_state,axis=1)\n",
    "                train_next_state_action = train_next_state_action.astype(np.int)\n",
    "\n",
    "                # Tells us whether game over or not\n",
    "                # We will multiply our rewards by this value\n",
    "                # to ensure we don't train on the last move\n",
    "                train_gameover = train_done == 0\n",
    "\n",
    "                # Q value of the next state based on action\n",
    "                train_next_state_values = target_q_next_state[range(batch_size), train_next_state_action]\n",
    "\n",
    "                # Reward from the action chosen in the train batch\n",
    "                actual_reward = train_reward + (y * train_next_state_values * train_gameover)\n",
    "                target_q[range(batch_size), train_action] = actual_reward\n",
    "\n",
    "                # Train the main model\n",
    "                loss = online_q_values.train_on_batch(train_state, target_q)\n",
    "                losses.append(loss)\n",
    "\n",
    "            # Update the target model with values from the main model\n",
    "            update_target_graph(online_q_values, target_q_values, tau)\n",
    "\n",
    "            #if (num_episode + 1) % save_every == 0:\n",
    "            #    # Save the model\n",
    "            #    online_q_values.model.save_weights(main_weights_file)\n",
    "            #    target_q_values.model.save_weights(target_weights_file)\n",
    "\n",
    "    \n",
    "    # Increment the episode\n",
    "    num_episode += 1\n",
    "    \n",
    "    experience_replay.add(episode_buffer.buffer)\n",
    "    num_steps.append(cur_step)\n",
    "    rewards.append(sum_rewards)\n",
    "    \n",
    "    if num_episode % print_every == 0:\n",
    "        # Print progress\n",
    "        mean_loss = np.mean(losses[-(print_every * num_epochs):])\n",
    "\n",
    "        print(\"Num episode: {} Mean reward: {:0.4f} Prob random: {:0.4f}, Loss: {:0.04f}\".format(\n",
    "            num_episode, np.mean(rewards[-print_every:]), prob_random, mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    if num_episode > pre_train_episodes:\n",
    "        # Training the network\n",
    "\n",
    "        if prob_random > prob_random_end:\n",
    "            # Drop the probability of a random action\n",
    "            prob_random -= prob_random_drop\n",
    "\n",
    "        if num_episode % update_freq == 0:\n",
    "            for num_epoch in range(num_epochs):\n",
    "                # Train batch is [[state,action,reward,next_state,done],...]\n",
    "                train_batch = experience_replay.sample(batch_size)\n",
    "\n",
    "                # Separate the batch into its components\n",
    "                train_state, train_action, train_reward, \\\n",
    "                    train_next_state, train_done = train_batch.T\n",
    "                    \n",
    "                # Convert the action array into an array of ints so they can be used for indexing\n",
    "                train_action = train_action.astype(np.int)\n",
    "\n",
    "                # Stack the train_state and train_next_state for learning\n",
    "                train_state = np.vstack(train_state)\n",
    "                train_next_state = np.vstack(train_next_state)\n",
    "\n",
    "                # Our predictions (actions to take) from the main Q network\n",
    "                target_q = target_qn.model.predict(train_state)\n",
    "                \n",
    "                # The Q values from our target network from the next state\n",
    "                target_q_next_state = main_qn.model.predict(train_next_state)\n",
    "                train_next_state_action = np.argmax(target_q_next_state,axis=1)\n",
    "                train_next_state_action = train_next_state_action.astype(np.int)\n",
    "                \n",
    "                # Tells us whether game over or not\n",
    "                # We will multiply our rewards by this value\n",
    "                # to ensure we don't train on the last move\n",
    "                train_gameover = train_done == 0\n",
    "\n",
    "                # Q value of the next state based on action\n",
    "                train_next_state_values = target_q_next_state[range(batch_size), train_next_state_action]\n",
    "\n",
    "                # Reward from the action chosen in the train batch\n",
    "                actual_reward = train_reward + (y * train_next_state_values * train_gameover)\n",
    "                target_q[range(batch_size), train_action] = actual_reward\n",
    "                \n",
    "                # Train the main model\n",
    "                loss = main_qn.model.train_on_batch(train_state, target_q)\n",
    "                losses.append(loss)\n",
    "                \n",
    "            # Update the target model with values from the main model\n",
    "            update_target_graph(main_qn.model, target_qn.model, tau)\n",
    "\n",
    "            if (num_episode + 1) % save_every == 0:\n",
    "                # Save the model\n",
    "                main_qn.model.save_weights(main_weights_file)\n",
    "                target_qn.model.save_weights(target_weights_file)\n",
    "    \n",
    "\n",
    "    # Increment the episode\n",
    "    num_episode += 1\n",
    "\n",
    "    experience_replay.add(episode_buffer.buffer)\n",
    "    num_steps.append(cur_step)\n",
    "    rewards.append(sum_rewards)\n",
    "        \n",
    "    if num_episode % print_every == 0:\n",
    "        # Print progress\n",
    "        mean_loss = np.mean(losses[-(print_every * num_epochs):])\n",
    "\n",
    "        print(\"Num episode: {} Mean reward: {:0.4f} Prob random: {:0.4f}, Loss: {:0.04f}\".format(\n",
    "            num_episode, np.mean(rewards[-print_every:]), prob_random, mean_loss))\n",
    "        if np.mean(rewards[-print_every:]) >= goal:\n",
    "            print(\"Training complete!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the main training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    step = global_step.eval()\n",
    "    if step >= n_steps:\n",
    "        break\n",
    "    iteration += 1\n",
    "    print(\"\\rIteration {}\\tTraining step {}/{} ({:.1f})%\\tLoss {:5f}\\tMean Max-Q {:5f}   \".format(\n",
    "        iteration, step, n_steps, step * 100 / n_steps, loss_val, mean_max_q), end=\"\")\n",
    "    if done: # game over, start again\n",
    "        obs = env.reset()\n",
    "        for skip in range(skip_start): # skip the start of each game\n",
    "            obs, reward, done, info = env.step(0)\n",
    "        state = preprocess_observation(obs)\n",
    "\n",
    "    # Online DQN evaluates what to do\n",
    "    q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "    action = epsilon_greedy(q_values, step)\n",
    "\n",
    "    # Online DQN plays\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    next_state = preprocess_observation(obs)\n",
    "\n",
    "    # Let's memorize what happened\n",
    "    replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "    state = next_state\n",
    "\n",
    "    # Compute statistics for tracking progress (not shown in the book)\n",
    "    total_max_q += q_values.max()\n",
    "    game_length += 1\n",
    "    if done:\n",
    "        mean_max_q = total_max_q / game_length\n",
    "        total_max_q = 0.0\n",
    "        game_length = 0\n",
    "\n",
    "    if iteration < training_start or iteration % training_interval != 0:\n",
    "        continue # only train after warmup period and at regular intervals\n",
    "\n",
    "    # Sample memories and use the target DQN to produce the target Q-Value\n",
    "    X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "        sample_memories(batch_size))\n",
    "    next_q_values = target_q_values.eval(\n",
    "        feed_dict={X_state: X_next_state_val})\n",
    "    max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "    y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "\n",
    "    # Train the online DQN\n",
    "    _, loss_val = sess.run([training_op, loss], feed_dict={\n",
    "        X_state: X_state_val, X_action: X_action_val, y: y_val})\n",
    "\n",
    "    # Regularly copy the online DQN to the target DQN\n",
    "    if step % copy_steps == 0:\n",
    "        copy_online_to_target.run()\n",
    "\n",
    "    # And save regularly\n",
    "    if step % save_steps == 0:\n",
    "        saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can interrupt the cell above at any time to test your agent using the cell below. You can then run the cell above once again, it will load the last parameters saved and resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "n_max_steps = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        state = preprocess_observation(obs)\n",
    "\n",
    "        # Online DQN evaluates what to do\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "        # Online DQN plays\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        frames.append(img)\n",
    "\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
