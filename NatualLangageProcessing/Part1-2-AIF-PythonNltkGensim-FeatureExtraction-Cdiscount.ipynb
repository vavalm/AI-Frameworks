{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies de l'intelligence Artificielle](https://github.com/wikistat/AI-Frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement Naturel du Langage (NLP) : Catégorisation de Produits Cdiscount\n",
    "\n",
    "Il s'agit d'une version simplifiée du concours proposé par Cdiscount et paru sur le site [datascience.net](https://www.datascience.net/fr/challenge). Les données d'apprentissage sont accessibles sur demande auprès de Cdiscount mais les solutions de l'échantillon test du concours ne sont pas et ne seront pas rendues publiques. Un échantillon test est donc construit pour l'usage de ce tutoriel.  L'objectif est de prévoir la catégorie d'un produit à partir de son descriptif (*text mining*). Seule la catégorie principale (1er niveau, 47 classes) est prédite au lieu des trois niveaux demandés dans le concours. L'objectif est plutôt de comparer les performances des méthodes et technologies en fonction de la taille de la base d'apprentissage ainsi que d'illustrer sur un exemple complexe le prétraitement de données textuelles. \n",
    "\n",
    "Le jeux de données complet (15M produits) permet un test en vrai grandeur du **passage à l'échelle volume** des phases de préparation (*munging*), vectorisation (hashage, TF-IDF) et d'apprentissage en fonction de la technologie utilisée.\n",
    "\n",
    "La synthèse des résultats obtenus est développée par [Besse et al. 2016](https://hal.archives-ouvertes.fr/hal-01350099) (section 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1-2 : Construction des caractéristiques ou *features*, Vectorisation et Word embedding.\n",
    "\n",
    "Les données textuelles ne peuvent pas être utilisés directement dans les différents algorithmes d'apprentissage statistiques. Nous allons voir dans ce tutoriel plusieurs techniques permettant de traduire les données textuelles sous formes de vecteur numérique : \n",
    "\n",
    "Fonction de vectorisation présente dans la librairie `scikit-learn` :\n",
    "\n",
    "* `One-Hot-Encoder`\n",
    "* `Tf-Idf`\n",
    "* `Hashing`\n",
    "\n",
    "Word Embedding  présente dans la librairie `gensim` :\n",
    "\n",
    "* `Word2Vec`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Importation des librairies utilisées\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import itertools\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement des données\n",
    "\n",
    "On télécharge les données de train et de validation néttoyé et racinisé dans le notebook précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_valid_clean_stem = pd.read_csv(\"data/cdiscount_valid_clean_stem.csv\").fillna(\"\")\n",
    "data_train_clean_stem = pd.read_csv(\"data/cdiscount_train_clean_stem.csv\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On créé un dossier dans lequel nous allons sauvegarder les DataFrame constitués des features que l'on va construire dans ce notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_OUTPUT_DIR = \"data/features\"\n",
    "if not(os.path.isdir(\"data/features\")):\n",
    "    os.mkdir(\"data/features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, en guise d'exemple et pour réduire le temps de calcul, on ne considère que la colonne *Description* de nos `DataFrame` générés dans le calepin précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_array = data_train_clean_stem[\"Description\"].values\n",
    "valid_array = data_valid_clean_stem[\"Description\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding\n",
    "\n",
    "* **One-Hot-Encoding**\n",
    "\n",
    "L'encodage  *One-Hot-Encoding* des données peut être effectué grâce à la classe `CountVectorizer`de **scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "extr_cv = CountVectorizer(binary=False)\n",
    "data_train_OHE = extr_cv.fit_transform(train_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** A quoi sert l'argument binary de la classe?\n",
    "\n",
    "**Q** Sous quel format sont stockées les vecteurs OHE? Pourquoi ce format est-il choisi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `get_feature_names` permet d'avoir accès à la liste des mots présents dans l'ensemble des lignes de l'array converti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = extr_cv.get_feature_names()\n",
    "N_vocabulary = len(vocabulary)\n",
    "print(\"Nombre de mots : %d\" %N_vocabulary )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Pour la première ligne de votre dataset train. retrouvez l'ensemble des mots constituant cette ligne à partir de l'objet `data_train_OHE`et de `vocabulary` ainsi que le nombre d'occurence de chacun de ces mots dans la ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:01:02.970929Z",
     "start_time": "2019-12-16T15:01:02.965778Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# %load solution/2_1.py\n",
    "ir = 0\n",
    "print(\"Liste des tokens racinisé de la première ligne : \" + train_array[0])\n",
    "rw = data_train_OHE.getrow(ir)\n",
    "pd.DataFrame([(v, vocabulary[v], k)  for k,v in zip(rw.data,rw.indices)], columns=[\"indices\",\"token\",\"weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La même transformation est appliqué sur l'échantillon de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid_OHE = extr_cv.transform(valid_array)\n",
    "data_valid_OHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que se passe-til pour les mots présents dans le dataset de validation mais qui ne sont pas présent dans le dataset d'apprentissage?\n",
    "\n",
    "**Q** Pourquoi on ne 're-fit' pas la Classe `CountVectorizer`l'échantillon de validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF¶\n",
    "\n",
    "* **TF-IDF**. Le TF-IDF permet de faire ressortir l'importance relative de chaque mot $m$ (ou couples de mots consécutifs) dans un texte-produit ou un descriptif $d$, par rapport à la liste entière des produits. La fonction $TF(m,d)$ compte le nombre d'occurences du mot $m$ dans le descriptif $d$. La fonction $IDF(m)$ mesure l'importance du terme dans l'ensemble des documents ou descriptifs en donnant plus de poids aux termes les moins fréquents car considérés comme les plus discriminants (motivation analogue à celle de la métrique du chi2 en anamlyse des correspondance). $IDF(m,l)=\\log\\frac{D}{f(m)}$ où $D$ est le nombre de documents, la taille de l'échantillon d'apprentissage, et $f(m)$ le nombre de documents ou descriptifs contenant le mot $m$. La nouvelle variable ou *features* est $V_m(l)=TF(m,l)\\times IDF(m,l)$.\n",
    "\n",
    "* Comme pour les transformations des variables quantitatives (centrage, réduction), la même transformation c'est-à-dire les mêmes pondérations, est calculée sur l'achantillon d'apprentissage et appliquée à celui de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utiliser la fonction `TfidfVectorizer` qui permet de parser également le texte\n",
    "\n",
    "Dans un premier temps, on fixe le paramètre `norm` = False afin de rendre les résultats plus explicite et analyser les sorties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(ngram_range=(1,1), norm = False)\n",
    "data_train_TFIDF = vec.fit_transform(train_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** A quoi sert l'argument ngram_range?\n",
    "\n",
    "Constatez que `data_train_TFIDF`est stocké sous le même format que `data_train_OHE` et que la taile du vocabulaire est la même."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vec.get_feature_names()\n",
    "N_vocabulary = len(vocabulary)\n",
    "N_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Pour la première ligne de votre dataset train. retrouvez l'ensemble des mots constituant cette ligne à partir de l'objet `data_train_TFIDF`et de `vocabulary` ainsi que la valeur de l'idf, du tf et du poids tfidf de chacun de ces mots dans la ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/2_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Commentez les valeurs de l'idf pour chacun des mots.\n",
    "\n",
    "**Q** Comment evolue les poids en changeant les paramètre *smooth idf* et *sublinear_tf* de la méthode `TfidfVectorizer`?\n",
    "\n",
    "**Exercice** Changez l'argument *ngram_range* de la méthode `TfidfVectorizer` et re-affichez les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique maintenant le `vectorizer` sur le jeu de données de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid_TFIDF = vec.transform(valid_array)\n",
    "data_valid_TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATENTION** Si le tf est recalculé pour chaque ligne, le même idf est utilisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/2_2bis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **Hashage**. Il permet de réduire l'espace des variables (taille du dictionnaire) en un nombre limité et fixé a priori `n_hash` de caractéristiques. Il repose sur la définition d'une fonction de hashage, $h$ qui à un indice $j$ défini dans l'espace des entiers naturels, renvoie un indice $i=h(j)$ dans dans l'espace réduit (1 à n_hash) des caractéristiques. Ainsi le poids de l'indice $i$, du nouvel espace, est l'association de tous les poids d'indice $j$ tels que $i=h(j)$ de l'espace originale. Ici, les poids sont associés d'après la méthode décrite par Weinberger et al. (2009).\n",
    "\n",
    "N.B. $h$ n'est pas généré aléatoirement. Ainsi pour un même fichier d'apprentissage (ou de test) et pour un même entier n_hash, le résultat de la fonction de hashage est identique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au contraire des classe `CountVectorizer`et `TfidfVectorizer`, la classe `FeatureHasher` prend en entré un dictionnaire d'occurence des mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict_array  = list(map(lambda x : collections.Counter(x.split(\" \")), train_array))\n",
    "train_dict_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "nb_hash = 300\n",
    "feathash = FeatureHasher(nb_hash)\n",
    "data_train_hash = feathash.fit_transform(train_dict_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constatez que `data_train_hash` est stocké sous le même format que `data_train_OHE` ou `data_train_TFIDF`.  \n",
    "\n",
    "**Q** Que dire cependant de sa dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule suivante permet d'afficher le poids de chacun des indices dans le nouvel espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = 0\n",
    "rw = data_train_hash.getrow(ir)\n",
    "print(\"Liste des tokens racinisé de la première ligne : \" + train_array[0])\n",
    "pd.DataFrame([(v, k)  for k,v in zip(rw.data,rw.indices)], columns=[\"indices\",\"weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que pouvez-vous dire des poids?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La taille de la matrice a donc été très réduit par rapport au vectorizer, TFIDF ou One-Hot-Enconding. Cependant, il n'y a pas de fonction inverse transform ce qui peut rendre le résultat moin compréhensible.\n",
    "\n",
    "\n",
    "Il est possible de combiner le `FeatureHasher`avec un autre vectorizer comme le TFIDF. \n",
    "\n",
    "C'est cette fois la classe `TFIDFTransformer` qui est utilisé. Celle ci ne ne considère pas des string en entré mais l'array en sortie de l'étape de Hashage. Les mots sont les `nb_hash`indices sélectionnés et le tf pour chaque individu sont les poids calculé par la fonction de hasage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vec =  TfidfTransformer(norm = False)\n",
    "data_train_HTfidf = vec.fit_transform(data_train_hash)\n",
    "data_train_HTfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = 0\n",
    "\n",
    "rw = data_train_HTfidf.getrow(ir)\n",
    "print(train_array[ir])\n",
    "pd.DataFrame([(ind, vec.idf_[ind], w/vec.idf_[ind], w)  for w,ind in zip(rw.data, rw.indices)], columns=[\"indices\",\"idf\",\"tf\",\"weight\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build and Save Vectorize Vector\n",
    "\n",
    "Afin de comparer l'effet de ces différentes vectorisation sur l'apprentissage, nous allons sauvegarder ces dernières sur les machines.\n",
    "\n",
    "De nombreux paramètres sont à régler ce qui entraine donc un un très grand nombre de combinaison.\n",
    "\n",
    "ici nous allons créer 4 jeu de données avec `CountVectorizer` et `TFIDF` chacun avec et sans hashage de taille 300.\n",
    "\n",
    "Il est evidemment possible de tester d'autre combinaison. Libre à vous de les tester ;) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction de Vectorisation\n",
    "\n",
    "on créé deux fonctions `vectorizer_train` and `apply_vectorizer` afin de générer automatiquement différent dataframe d'apprentissage et de validation vectorisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizer_train(df, columns=['Description'], nb_hash=None, nb_gram = 1, vectorizer = \"tfidf\" , binary = False):\n",
    "    \n",
    "    data_array = [\" \".join(line) for line in df[columns].values]\n",
    "    \n",
    "    # Hashage\n",
    "    if nb_hash is None:\n",
    "        feathash = None\n",
    "        if vectorizer == \"tfidf\":\n",
    "            vec = TfidfVectorizer(ngram_range=(1,nb_gram))\n",
    "            data_vec = vec.fit_transform(data_array)\n",
    "        else:\n",
    "            vec = CountVectorizer(binary=binary)\n",
    "            data_vec = vec.fit_transform(data_array)\n",
    "    else:\n",
    "        data_dic_array = [collections.Counter(line.split(\" \")) for line in data_array]\n",
    "        feathash = FeatureHasher(nb_hash)\n",
    "        data_hash = feathash.fit_transform(data_dic_array)\n",
    "        \n",
    "        if vectorizer==\"tfidf\":\n",
    "            vec =  TfidfTransformer()\n",
    "            data_vec =  vec.fit_transform(data_hash)\n",
    "        else:\n",
    "            vec = None\n",
    "            data_vec = data_hash\n",
    "\n",
    "    return vec, feathash, data_vec\n",
    "\n",
    "\n",
    "\n",
    "def apply_vectorizer(df, vec, feathash, columns =['Description', 'Libelle', 'Marque']):\n",
    "    \n",
    "    data_array = [\" \".join(line) for line in df[columns].values]\n",
    "    \n",
    "    #Hashage\n",
    "    if feathash is None:\n",
    "        data_hash = data_array\n",
    "    else:\n",
    "        data_dic_array = [collections.Counter(line.split(\" \")) for line in data_array]\n",
    "        data_hash = feathash.transform(data_dic_array)\n",
    "    \n",
    "    if vec is None:\n",
    "        data_vec = data_hash\n",
    "    else:\n",
    "        data_vec = vec.transform(data_hash)\n",
    "    return data_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [[None, \"count\"],\n",
    "              [300, \"count\"],\n",
    "              [None, \"tfidf\"],\n",
    "              [300, \"tfidf\"]]\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "for nb_hash, vectorizer in parameters:\n",
    "    ts = time.time()\n",
    "    vec, feathash, data_train_vec = vectorizer_train(data_train_clean_stem, nb_hash=nb_hash, vectorizer = vectorizer)\n",
    "    data_valid_vec = apply_vectorizer(data_valid_clean_stem, vec, feathash)\n",
    "    te = time.time()\n",
    "    \n",
    "    print(\"nb_hash : \" + str(nb_hash) + \", vectorizer : \" + str(vectorizer))\n",
    "    print(\"Runing time for vectorization : %.1f seconds\" %(te-ts))\n",
    "    print(\"Train shape : \" + str(data_train_vec.shape))\n",
    "    print(\"Valid shape : \" + str(data_valid_vec.shape))\n",
    "\n",
    "    \n",
    "    sparse.save_npz(DATA_OUTPUT_DIR +\"/vec_train_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer), data_train_vec)\n",
    "    sparse.save_npz(DATA_OUTPUT_DIR +\"/vec_valid_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer), data_valid_vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Dans cette partie, des modèles `Word2Vec`Seront créés à l'aide de la librairie [**gensim**](https://radimrehurek.com/gensim/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Word2Vec model\n",
    "\n",
    "La fonction `gensim.models.Word2Vec`qui permet de construire des modèle Word2Vec prend en entrée une liste de tokens. On tranformer donc nos données dans un premier temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_array_token = [line.split(\" \") for line in train_array]\n",
    "valid_array_token = [line.split(\" \") for line in valid_array]\n",
    "train_array_token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction contient un grand nombre d' [arguments](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec). Le but de ce TP n'est pas d'optimiser les paramètres de ce modèle mais de les comprendre. Nous allons donc fixer quelques arguments par défault : \n",
    "\n",
    "* Features_dimension = 300 : Dimension de l'espace des features (d'*embedding*) qui sera crée.\n",
    "* hs = 0\n",
    "* negative = 10\n",
    "\n",
    "**Q** A quoi servent les arguments *hs* et *negative*? Quels influences ces arguments ont sur le modèle avec les valeurs définies ici?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Features_dimension = 300\n",
    "hs = 0\n",
    "negative = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer deux modèles :\n",
    "\n",
    "* Un modèle **skip-sgram**, sg = 1\n",
    "* Un modèle **CBOW**, sg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = 1\n",
    "print(\"Start learning skip-gram Word2Vec\")\n",
    "ts = time.time()\n",
    "model_sg = gensim.models.Word2Vec(train_array_token, sg=sg, hs=hs, negative=negative, min_count=1, size=Features_dimension)\n",
    "te = time.time()\n",
    "t_learning = te-ts\n",
    "print(\"Learning time : %.2f Word2Vec\" %t_learning)\n",
    "\n",
    "\n",
    "sg = 0\n",
    "print(\"Start learning CBOW Word2Vec\")\n",
    "ts = time.time()\n",
    "model_cbow = gensim.models.Word2Vec(train_array_token, sg=sg, hs=hs, negative=negative, min_count=1, size=Features_dimension)\n",
    "te = time.time()\n",
    "t_learning = te-ts\n",
    "print(\"Learning time : %.2f Word2Vec\" %t_learning)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire du temps d'apprentissage de ces deux modèles? D'ou vient cette différence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained Model\n",
    "\n",
    "Comme pour les réseaux de convolution, des modèles pré-entrainés de Word2Vec éxistent également. \n",
    "Le plus célèbre et le plus utilisé étant [`GoogleNewsVectors`](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit) appris sur plus de 100 milliard de mots à partir des articles de GoogleNews. Cependant ce modèle est en anglais, et n'est donc )as utile ici.\n",
    "\n",
    "\n",
    "On utilisera des modèle appris dans le projet suivant [https://github.com/Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors) appris sur 1Giga d'articles de wikipedia en mode **Skip-Gram*\n",
    "\n",
    "Vous pouvez télécharger ce modèle en suivant ce [lien](https://we.tl/t-wo54Y3sVBt). Dezipez-le puis téléchargez le modèle en indiquant la direction du fichier \"fr/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_online_dir = \"data/fr/fr.bin\"\n",
    "#model_online_dir = \"ACOMPLETER/fr.bin\"\n",
    "model_online = gensim.models.Word2Vec.load(model_online_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propriété du modèle\n",
    "\n",
    "Nous allons maintenant comparer quelques propriétés de chacun des trois modèles à notre disposition (*CBOW*, *Skip-Gram* et le modèle *online*)\n",
    "\n",
    "Les modèles que nous avons appris l'ont été sur les mots *racinisé*. Ainsi, nous allons avoir besoin de la racine des mots pour tester les différentes propriétés du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "stemmer=nltk.stem.SnowballStemmer('french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most similar world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `most_similar` de **gensim** permet de retrouver les mots les plus proches à un ou une combinaison de mots données en argument.\n",
    "\n",
    "**Q** A l'aide de la [documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar) répondez aux questions suivantes. Quelle est la mesure de similarité utilisée? Dans quel espace est-elle utilisé? Comment fonctionne la fonction lorsque plusieurs mots sont passés en paramètres? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1 mot\n",
    "\n",
    "\n",
    "**Exercice** Pour chacun de ces trois modèles, affichez les sorties de la fonction 'most_similar' pour le mot *homme*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load solution/2_3.py\n",
    "%load solution/2_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comparez la qualité de prévision des modèles que nous avons entrainés sur le jeu de données 'Cdiscount' avec celui appris online. Que pouvez-vous en dire? \n",
    "\n",
    "**Q** Comparez les prévisions des deux modèles que nous avons entrainés. Que pouvez-vous en dire?\n",
    "\n",
    "**Exercice** Affichez maintenant les sorties de la fonction 'most_similar' pour le mot *femme*. \n",
    "\n",
    "**Exercice** Affichez les sorties de la fonction 'most_similar' pour des mots propre au jeu de données (ex. *xbox*, *pantalon*,..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combinaison de mots\n",
    "\n",
    "**Exercice** Pour chacun de ces trois modèles, affichez les sorties de la fonction 'most_similar' pour l'opération suivante : *roi* + *femme* - *homme* à l'aide des arguments *positive* et *negative* de la fonction. Commentez encore une fois la qualité de sortie des différents modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "UsageError: Missing filename, URL, input history range, macro, or element in the user namespace.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "%load solution/2_4.py\n",
    "%load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Testez d'autres combinaisons si vous le souhaitez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict output word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `predict_output_word` de **gensim** permet de retrouver les mots prédit par le modèle à partir d'un mot ou d'une combinaison de mots données en argument.\n",
    "\n",
    "**Exercice**  Affichez la prédiction des trois modèles pour des mots/combinaisons de mots communs (*homme*, *femme*) ou plus propre au jeu de données étudiés. (*coque*-*de*-*téléphone*)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load solution/2_5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Features\n",
    "\n",
    "Nous allons maintenant créer des matrices de features à partir des modèles de **Word2Vec** générés précédemment dans un but de prédiction. \n",
    "\n",
    "La modèles créés permettent d'obtenir pour chaque mot `x`, sa représentation dans l'espace d'embeddings de la manière suivante : \n",
    "\n",
    "*x_feature = model_word2vec[x]* \n",
    "\n",
    "Dans notre problématique, les individus que nous cherchons à classer sont des descriptions représentées par une liste de token à l'issue du nettoyage du calepin précédent.  Il exsite donc plusieurs façon de représenter ces individus à l'aide de modèle **Word2Vec**. \n",
    "\n",
    "1. Moyenne des vecteurs dans l'espace des features des différents mots/token de la description.\n",
    "2. Moyenne pondérées des vecteurs dans l'espace des features des différents mots/token de la description en fonction de l'occurence de chacun de ces mots/tokens dans la description.\n",
    "3. Moyenne pondérées par des poids calculés à l'aide du TF-IDF.\n",
    "4. etc...\n",
    "\n",
    "C'est la seconde solution qui sera utilisée ici. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions détaillées ci-dessous permettent de :\n",
    "    \n",
    "* `get_features_mean` : retourne le vecteur moyen dans l'espace d'embedding, des projections des mots/tokens composant *lines*\n",
    "* `get_matrix_features_means` : applique la fonction `get_features_mean` sur tous les éléments de la matrice *X*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_mean(lines, model, f_size):\n",
    "    features = [model[x] for x  in lines if x in model]\n",
    "    if features == []:   \n",
    "        fm =np.ones(f_size)\n",
    "    else :\n",
    "        fm = np.mean(features,axis=0)\n",
    "    return fm\n",
    "\n",
    "def get_matrix_features_means(X, model, f_size):\n",
    "    X_embedded_ = list(map(lambda x : get_features_mean(x, model, f_size), X))\n",
    "    X_embedded = np.vstack(X_embedded_)\n",
    "    return X_embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "X_embedded_train_cbow = get_matrix_features_means(train_array_token, model_cbow, Features_dimension)\n",
    "te = time.time()\n",
    "t_build = te-ts\n",
    "#np.save(embedded_train_dir, X_embedded_train)\n",
    "print(\"Time conversion : %d seconds\"%t_build)\n",
    "print(\"Shape Matrix : (%d,%d)\"%X_embedded_train_cbow.shape)\n",
    "np.save(DATA_OUTPUT_DIR +\"/embedded_train_cbow\", X_embedded_train_cbow)\n",
    "\n",
    "ts = time.time()\n",
    "X_embedded_valid_cbow = get_matrix_features_means(valid_array_token, model_cbow, Features_dimension)\n",
    "te = time.time()\n",
    "t_build = te-ts\n",
    "#np.save(embedded_train_dir, X_embedded_train)\n",
    "print(\"Time conversion : %d seconds\"%t_build)\n",
    "print(\"Shape Matrix : (%d,%d)\"%X_embedded_valid_cbow.shape)\n",
    "np.save(DATA_OUTPUT_DIR +\"/embedded_valid_cbow\", X_embedded_valid_cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "X_embedded_train_sg = get_matrix_features_means(train_array_token, model_sg, Features_dimension)\n",
    "te = time.time()\n",
    "t_build = te-ts\n",
    "#np.save(embedded_train_dir, X_embedded_train)\n",
    "print(\"Time conversion : %d seconds\"%t_build)\n",
    "print(\"Shape Matrix : (%d,%d)\"%X_embedded_train_sg.shape)\n",
    "np.save(DATA_OUTPUT_DIR +\"/embedded_train_sg\", X_embedded_train_sg)\n",
    "\n",
    "ts = time.time()\n",
    "X_embedded_valid_sg = get_matrix_features_means(valid_array_token, model_sg, Features_dimension)\n",
    "te = time.time()\n",
    "t_build = te-ts\n",
    "#np.save(embedded_train_dir, X_embedded_train)\n",
    "print(\"Time conversion : %d seconds\"%t_build)\n",
    "print(\"Shape Matrix : (%d,%d)\"%X_embedded_valid_sg.shape)\n",
    "np.save(DATA_OUTPUT_DIR +\"/embedded_valid_sg\", X_embedded_valid_sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_valid_clean = pd.read_csv(\"data/cdiscount_valid_clean.csv\").fillna(\"\")\n",
    "data_train_clean = pd.read_csv(\"data/cdiscount_train_clean.csv\").fillna(\"\")\n",
    "\n",
    "train_array_token_nostem = [line.split(\" \") for line in data_train_clean[\"Description\"].values]\n",
    "valid_array_token_nostem = [line.split(\" \") for line in data_valid_clean[\"Description\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "X_embedded_train_online = get_matrix_features_means(train_array_token_nostem, model_online, Features_dimension)\n",
    "te = time.time()\n",
    "t_build = te-ts\n",
    "#np.save(embedded_train_dir, X_embedded_train)\n",
    "print(\"Time conversion : %d seconds\"%t_build)\n",
    "print(\"Shape Matrix : (%d,%d)\"%X_embedded_train_online.shape)\n",
    "np.save(DATA_OUTPUT_DIR +\"/embedded_train_online\", X_embedded_train_online)\n",
    "\n",
    "ts = time.time()\n",
    "X_embedded_valid_online = get_matrix_features_means(valid_array_token_nostem, model_online, Features_dimension)\n",
    "te = time.time()\n",
    "t_build = te-ts\n",
    "#np.save(embedded_train_dir, X_embedded_train)\n",
    "print(\"Time conversion : %d seconds\"%t_build)\n",
    "print(\"Shape Matrix : (%d,%d)\"%X_embedded_valid_online.shape)\n",
    "np.save(DATA_OUTPUT_DIR +\"/embedded_valid_online\", X_embedded_valid_online)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "279px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}