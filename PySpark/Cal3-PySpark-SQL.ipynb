{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IA Framework.\n",
    "## Lab 1  - Introduction to Pyspark.\n",
    "#### Part 3: *DataFrame*  ",
    " <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a> [SQL](http://spark.apache.org/sql/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résume**: This notebook continue the introduction to [Spark](https://spark.apache.org/) trough  [`PySpark`](http://spark.apache.org/docs/latest/api/python/) API. In this notebook we will learn to manipulate ***Spark's Dataframe***, which will be the standard class and will replace RDD for Spark 3.0 release. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue to used [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) dataset in this TP. \n",
    "\n",
    "In this TP we will also used the names of the columns as the `Dataframe` class will be able to handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('kddcup.names', <http.client.HTTPMessage at 0x7f0126f459d0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH=\"\" \n",
    "import urllib.request\n",
    "urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names\",DATA_PATH+\"kddcup.names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the file of column's names and create a list of those names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = open(DATA_PATH+\"kddcup.names\",\"r\").readlines()\n",
    "col_names = [k.split(\":\")[0] for k in file_names[1:]]+[\"interactions\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  *Spark SQL*\n",
    "\n",
    "\n",
    "*Spark SQL* is a Spark library which allows to manipulate structured data, while RDD does not. Indeed Spark will see no differences between an RDD of list or an RDD of dictionaries. \n",
    "\n",
    "With *Spark SQL* the data are organised according to a schema. A schema will define the type of variable that will enable faster computation.\n",
    "\n",
    "They are various way to build and manipulate those structured object :SQL request, `Dataset` API  (only for java and scala) or `pyspark`' `Dataframe` that will be used all along this TP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen in the first notebook that Spark needs a context to be used :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to manipulate `Dataframe` we will need to define a new context : the `SQLContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f0124805110>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *DataFrame* construction\n",
    "\n",
    "A Spark's `Dataframe` is a collection of distributed data organised by column.\n",
    "\n",
    "It is similar to R's *Dataframe* or Python pandas' *Dataframe*.They can be build from various sources such as  : *Hive*, *json*, *xml*, *parquet*, *cassandra*... \n",
    "\n",
    "We will see how to build DataFrame from `RDD`, `.csv` and  `pandas`' *DataFrame*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From  a RDD\n",
    "\n",
    "We first create the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = DATA_PATH+\"kddcup.data_10_percent.gz\"\n",
    "string_rdd = sc.textFile(data_file)\n",
    "list_rdd = string_rdd.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the Schema\n",
    "\n",
    "We will first specify the schema in the RDD before we convert it into a `DataFrame`.  ",
    "The schema is defined as a `StructType` object composed of `StructField` which will defined the type of the different field.\n",
    "\n",
    "We will create a DataFrame that contains the following columns : *duration,rotocol_type, service, flag, src_bytes, dst_bytes, interactions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "fields = [StructField(\"duration\", IntegerType(), True),\n",
    "          StructField(\"protocol_type\", StringType(), True),\n",
    "          StructField(\"service\", StringType(), True),\n",
    "          StructField(\"flag\", StringType(), True),\n",
    "          StructField(\"src_bytes\", IntegerType(), True),\n",
    "          StructField(\"dst_bytes\", IntegerType(), True),\n",
    "          StructField(\"interactions\", StringType(), True)]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then easily create the `DataFrame` with the `createDataFrame` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[duration: int, protocol_type: string, service: string, flag: string, src_bytes: int, dst_bytes: int, interactions: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subList_rdd = list_rdd.map(lambda p: (int(p[0]), p[1], p[2], p[3], int(p[4]), int(p[5]), p[-1]))\n",
    "df_rdd = sqlContext.createDataFrame(subList_rdd, schema)\n",
    "df_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(duration=0, protocol_type='tcp', service='http', flag='SF', src_bytes=181, dst_bytes=5450, interactions='normal.'),\n",
       " Row(duration=0, protocol_type='tcp', service='http', flag='SF', src_bytes=239, dst_bytes=486, interactions='normal.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring the Schema\n",
    "\n",
    "You can also create a `DataFrame` from a RDD by inferring the schema. \n",
    "It requires that the RDD is composed of `Row` objects that are dictionary-like object in Spark.\n",
    "The type of value from each entry of the Row is deducted from the first row which need to be correctly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "row_rdd = list_rdd.map(lambda p: Row(\n",
    "    duration=int(p[0]), \n",
    "    protocol_type=p[1],\n",
    "    service=p[2],\n",
    "    flag=p[3],\n",
    "    src_bytes=int(p[4]),\n",
    "    dst_bytes=int(p[5]),\n",
    "    interactions = p[-1],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the RDD is created you can call the `createDataFrame` function without specifying any schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dst_bytes=5450, duration=0, flag='SF', interactions='normal.', protocol_type='tcp', service='http', src_bytes=181),\n",
       " Row(dst_bytes=486, duration=0, flag='SF', interactions='normal.', protocol_type='tcp', service='http', src_bytes=239)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rdd_1 = sqlContext.createDataFrame(row_rdd)\n",
    "df_rdd_1.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A schema can be displayed with the `printSchema` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- interactions: string (nullable = true)\n",
      " |-- protocol_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rdd_1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a `.csv`\n",
    "\n",
    "When the source file is structured in a given format (*parquet*, *json*, *csv*),the `spark.read.load` function allow to directly infer the schema. \n",
    "\n",
    "The *kddcup.data_10_percent.gz* file is organised as a`.csv`. The *dataframe* can then be build directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, _c1: string, _c2: string, _c3: string, _c4: int, _c5: int, _c6: int, _c7: int, _c8: int, _c9: int, _c10: int, _c11: int, _c12: int, _c13: int, _c14: int, _c15: int, _c16: int, _c17: int, _c18: int, _c19: int, _c20: int, _c21: int, _c22: int, _c23: int, _c24: double, _c25: double, _c26: double, _c27: double, _c28: double, _c29: double, _c30: double, _c31: int, _c32: int, _c33: double, _c34: double, _c35: double, _c36: double, _c37: double, _c38: double, _c39: double, _c40: double, _c41: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv = sqlContext.read.load(data_file, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"False\")\n",
    "df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now specify the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv=df_csv.toDF(*col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(duration=0, protocol_type='tcp', service='http', flag='SF', src_bytes=181, dst_bytes=5450, land=0, wrong_fragment=0, urgent=0, hot=0, num_failed_logins=0, logged_in=1, num_compromised=0, root_shell=0, su_attempted=0, num_root=0, num_file_creations=0, num_shells=0, num_access_files=0, num_outbound_cmds=0, is_host_login=0, is_guest_login=0, count=8, srv_count=8, serror_rate=0.0, srv_serror_rate=0.0, rerror_rate=0.0, srv_rerror_rate=0.0, same_srv_rate=1.0, diff_srv_rate=0.0, srv_diff_host_rate=0.0, dst_host_count=9, dst_host_srv_count=9, dst_host_same_srv_rate=1.0, dst_host_diff_srv_rate=0.0, dst_host_same_src_port_rate=0.11, dst_host_srv_diff_host_rate=0.0, dst_host_serror_rate=0.0, dst_host_srv_serror_rate=0.0, dst_host_rerror_rate=0.0, dst_host_srv_rerror_rate=0.0, interactions='normal.'),\n",
       " Row(duration=0, protocol_type='tcp', service='http', flag='SF', src_bytes=239, dst_bytes=486, land=0, wrong_fragment=0, urgent=0, hot=0, num_failed_logins=0, logged_in=1, num_compromised=0, root_shell=0, su_attempted=0, num_root=0, num_file_creations=0, num_shells=0, num_access_files=0, num_outbound_cmds=0, is_host_login=0, is_guest_login=0, count=8, srv_count=8, serror_rate=0.0, srv_serror_rate=0.0, rerror_rate=0.0, srv_rerror_rate=0.0, same_srv_rate=1.0, diff_srv_rate=0.0, srv_diff_host_rate=0.0, dst_host_count=19, dst_host_srv_count=19, dst_host_same_srv_rate=1.0, dst_host_diff_srv_rate=0.0, dst_host_same_src_port_rate=0.05, dst_host_srv_diff_host_rate=0.0, dst_host_serror_rate=0.0, dst_host_srv_serror_rate=0.0, dst_host_rerror_rate=0.0, dst_host_srv_rerror_rate=0.0, interactions='normal.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a `pandas`' *DataFrame* \n",
    "\n",
    "\n",
    "Convert `pandas`' *DataFrame*  to a `PysSpark`'s `DataFrame` require the use of *pyarrow* library that enable to move object from *JVM* to *python*.\n",
    "\n",
    "To use it, you will need to enable its execution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[duration: bigint, protocol_type: string, service: string, flag: string, src_bytes: bigint, dst_bytes: bigint, land: bigint, wrong_fragment: bigint, urgent: bigint, hot: bigint, num_failed_logins: bigint, logged_in: bigint, num_compromised: bigint, root_shell: bigint, su_attempted: bigint, num_root: bigint, num_file_creations: bigint, num_shells: bigint, num_access_files: bigint, num_outbound_cmds: bigint, is_host_login: bigint, is_guest_login: bigint, count: bigint, srv_count: bigint, serror_rate: double, srv_serror_rate: double, rerror_rate: double, srv_rerror_rate: double, same_srv_rate: double, diff_srv_rate: double, srv_diff_host_rate: double, dst_host_count: bigint, dst_host_srv_count: bigint, dst_host_same_srv_rate: double, dst_host_diff_srv_rate: double, dst_host_same_src_port_rate: double, dst_host_srv_diff_host_rate: double, dst_host_serror_rate: double, dst_host_srv_serror_rate: double, dst_host_rerror_rate: double, dst_host_srv_rerror_rate: double, interactions: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_df = pd.read_csv(DATA_PATH+\"kddcup.data_10_percent.gz\", sep=\",\", names=col_names)\n",
    "df = sqlContext.createDataFrame(pandas_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(duration=0, protocol_type='tcp', service='http', flag='SF', src_bytes=181, dst_bytes=5450, land=0, wrong_fragment=0, urgent=0, hot=0, num_failed_logins=0, logged_in=1, num_compromised=0, root_shell=0, su_attempted=0, num_root=0, num_file_creations=0, num_shells=0, num_access_files=0, num_outbound_cmds=0, is_host_login=0, is_guest_login=0, count=8, srv_count=8, serror_rate=0.0, srv_serror_rate=0.0, rerror_rate=0.0, srv_rerror_rate=0.0, same_srv_rate=1.0, diff_srv_rate=0.0, srv_diff_host_rate=0.0, dst_host_count=9, dst_host_srv_count=9, dst_host_same_srv_rate=1.0, dst_host_diff_srv_rate=0.0, dst_host_same_src_port_rate=0.11, dst_host_srv_diff_host_rate=0.0, dst_host_serror_rate=0.0, dst_host_srv_serror_rate=0.0, dst_host_rerror_rate=0.0, dst_host_srv_rerror_rate=0.0, interactions='normal.'),\n",
       " Row(duration=0, protocol_type='tcp', service='http', flag='SF', src_bytes=239, dst_bytes=486, land=0, wrong_fragment=0, urgent=0, hot=0, num_failed_logins=0, logged_in=1, num_compromised=0, root_shell=0, su_attempted=0, num_root=0, num_file_creations=0, num_shells=0, num_access_files=0, num_outbound_cmds=0, is_host_login=0, is_guest_login=0, count=8, srv_count=8, serror_rate=0.0, srv_serror_rate=0.0, rerror_rate=0.0, srv_rerror_rate=0.0, same_srv_rate=1.0, diff_srv_rate=0.0, srv_diff_host_rate=0.0, dst_host_count=19, dst_host_srv_count=19, dst_host_same_srv_rate=1.0, dst_host_diff_srv_rate=0.0, dst_host_same_src_port_rate=0.05, dst_host_srv_diff_host_rate=0.0, dst_host_serror_rate=0.0, dst_host_srv_serror_rate=0.0, dst_host_rerror_rate=0.0, dst_host_srv_rerror_rate=0.0, interactions='normal.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requête SQL\n",
    "\n",
    "\n",
    "*SparkSQL* enables to apply SQL request and return the result to a `DataFrame`.\n",
    "\n",
    "We do not have a SQL database available here. But the sql context to apply sql request on existing `DataFrame`.\n",
    "We need for that to register it on a  *SQL temporary view* format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.createOrReplaceTempView(\"interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select *tcp* interaction without transfer (dst_bytes = 0) and with duration above 1second.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[duration: int, dst_bytes: int]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcp_interactions = sqlContext.sql(\"\"\"\n",
    "    SELECT duration, dst_bytes FROM interactions WHERE protocol_type = 'tcp' AND duration > 1000 AND dst_bytes \n",
    "    = 0\"\"\")\n",
    "tcp_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The request is a *DataFrames* object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[duration: int, dst_bytes: int]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcp_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are still using Spark! The above execution are lazy. You need to call action's function to actually apply the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcp_interactions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5059, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5056, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5039, Dest. bytes: 0\n",
      "Duration: 5062, Dest. bytes: 0\n",
      "Duration: 5041, Dest. bytes: 0\n",
      "Duration: 5056, Dest. bytes: 0\n",
      "Duration: 5064, Dest. bytes: 0\n",
      "Duration: 5043, Dest. bytes: 0\n",
      "Duration: 5061, Dest. bytes: 0\n",
      "Duration: 5049, Dest. bytes: 0\n",
      "Duration: 5061, Dest. bytes: 0\n",
      "Duration: 5048, Dest. bytes: 0\n",
      "Duration: 5047, Dest. bytes: 0\n",
      "Duration: 5044, Dest. bytes: 0\n",
      "Duration: 5063, Dest. bytes: 0\n",
      "Duration: 5068, Dest. bytes: 0\n",
      "Duration: 5062, Dest. bytes: 0\n",
      "Duration: 5046, Dest. bytes: 0\n",
      "Duration: 5052, Dest. bytes: 0\n",
      "Duration: 5044, Dest. bytes: 0\n",
      "Duration: 5054, Dest. bytes: 0\n",
      "Duration: 5039, Dest. bytes: 0\n",
      "Duration: 5058, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5032, Dest. bytes: 0\n",
      "Duration: 5063, Dest. bytes: 0\n",
      "Duration: 5040, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5066, Dest. bytes: 0\n",
      "Duration: 5044, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5036, Dest. bytes: 0\n",
      "Duration: 5055, Dest. bytes: 0\n",
      "Duration: 2426, Dest. bytes: 0\n",
      "Duration: 5047, Dest. bytes: 0\n",
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5037, Dest. bytes: 0\n",
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5062, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5053, Dest. bytes: 0\n",
      "Duration: 5064, Dest. bytes: 0\n",
      "Duration: 5044, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5033, Dest. bytes: 0\n",
      "Duration: 5066, Dest. bytes: 0\n",
      "Duration: 5063, Dest. bytes: 0\n",
      "Duration: 5056, Dest. bytes: 0\n",
      "Duration: 5042, Dest. bytes: 0\n",
      "Duration: 5063, Dest. bytes: 0\n",
      "Duration: 5060, Dest. bytes: 0\n",
      "Duration: 5056, Dest. bytes: 0\n",
      "Duration: 5049, Dest. bytes: 0\n",
      "Duration: 5043, Dest. bytes: 0\n",
      "Duration: 5039, Dest. bytes: 0\n",
      "Duration: 5041, Dest. bytes: 0\n",
      "Duration: 42448, Dest. bytes: 0\n",
      "Duration: 42088, Dest. bytes: 0\n",
      "Duration: 41065, Dest. bytes: 0\n",
      "Duration: 40929, Dest. bytes: 0\n",
      "Duration: 40806, Dest. bytes: 0\n",
      "Duration: 40682, Dest. bytes: 0\n",
      "Duration: 40571, Dest. bytes: 0\n",
      "Duration: 40448, Dest. bytes: 0\n",
      "Duration: 40339, Dest. bytes: 0\n",
      "Duration: 40232, Dest. bytes: 0\n",
      "Duration: 40121, Dest. bytes: 0\n",
      "Duration: 36783, Dest. bytes: 0\n",
      "Duration: 36674, Dest. bytes: 0\n",
      "Duration: 36570, Dest. bytes: 0\n",
      "Duration: 36467, Dest. bytes: 0\n",
      "Duration: 36323, Dest. bytes: 0\n",
      "Duration: 36204, Dest. bytes: 0\n",
      "Duration: 32038, Dest. bytes: 0\n",
      "Duration: 31925, Dest. bytes: 0\n",
      "Duration: 31809, Dest. bytes: 0\n",
      "Duration: 31709, Dest. bytes: 0\n",
      "Duration: 31601, Dest. bytes: 0\n",
      "Duration: 31501, Dest. bytes: 0\n",
      "Duration: 31401, Dest. bytes: 0\n",
      "Duration: 31301, Dest. bytes: 0\n",
      "Duration: 31194, Dest. bytes: 0\n",
      "Duration: 31061, Dest. bytes: 0\n",
      "Duration: 30935, Dest. bytes: 0\n",
      "Duration: 30835, Dest. bytes: 0\n",
      "Duration: 30735, Dest. bytes: 0\n",
      "Duration: 30619, Dest. bytes: 0\n",
      "Duration: 30518, Dest. bytes: 0\n",
      "Duration: 30418, Dest. bytes: 0\n",
      "Duration: 30317, Dest. bytes: 0\n",
      "Duration: 30217, Dest. bytes: 0\n",
      "Duration: 30077, Dest. bytes: 0\n",
      "Duration: 25420, Dest. bytes: 0\n",
      "Duration: 22921, Dest. bytes: 0\n",
      "Duration: 22821, Dest. bytes: 0\n",
      "Duration: 22721, Dest. bytes: 0\n",
      "Duration: 22616, Dest. bytes: 0\n",
      "Duration: 22516, Dest. bytes: 0\n",
      "Duration: 22416, Dest. bytes: 0\n",
      "Duration: 22316, Dest. bytes: 0\n",
      "Duration: 22216, Dest. bytes: 0\n",
      "Duration: 21987, Dest. bytes: 0\n",
      "Duration: 21887, Dest. bytes: 0\n",
      "Duration: 21767, Dest. bytes: 0\n",
      "Duration: 21661, Dest. bytes: 0\n",
      "Duration: 21561, Dest. bytes: 0\n",
      "Duration: 21455, Dest. bytes: 0\n",
      "Duration: 21334, Dest. bytes: 0\n",
      "Duration: 21223, Dest. bytes: 0\n",
      "Duration: 21123, Dest. bytes: 0\n",
      "Duration: 20983, Dest. bytes: 0\n",
      "Duration: 14682, Dest. bytes: 0\n",
      "Duration: 14420, Dest. bytes: 0\n",
      "Duration: 14319, Dest. bytes: 0\n",
      "Duration: 14198, Dest. bytes: 0\n",
      "Duration: 14098, Dest. bytes: 0\n",
      "Duration: 13998, Dest. bytes: 0\n",
      "Duration: 13898, Dest. bytes: 0\n",
      "Duration: 13796, Dest. bytes: 0\n",
      "Duration: 13678, Dest. bytes: 0\n",
      "Duration: 13578, Dest. bytes: 0\n",
      "Duration: 13448, Dest. bytes: 0\n",
      "Duration: 13348, Dest. bytes: 0\n",
      "Duration: 13241, Dest. bytes: 0\n",
      "Duration: 13141, Dest. bytes: 0\n",
      "Duration: 13033, Dest. bytes: 0\n",
      "Duration: 12933, Dest. bytes: 0\n",
      "Duration: 12833, Dest. bytes: 0\n",
      "Duration: 12733, Dest. bytes: 0\n",
      "Duration: 12001, Dest. bytes: 0\n",
      "Duration: 5678, Dest. bytes: 0\n",
      "Duration: 5010, Dest. bytes: 0\n",
      "Duration: 1298, Dest. bytes: 0\n",
      "Duration: 1031, Dest. bytes: 0\n",
      "Duration: 36438, Dest. bytes: 0\n"
     ]
    }
   ],
   "source": [
    "# Sortie des durées avec les dst_bytes\n",
    "tcp_interactions_out = tcp_interactions.rdd.map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))\n",
    "for ti_out in tcp_interactions_out.collect():\n",
    "  print (ti_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Operations on *DataFrame* \n",
    "### Elementary operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*select* allow to extract columns of a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(interactions='normal.'),\n",
       " Row(interactions='normal.'),\n",
       " Row(interactions='normal.'),\n",
       " Row(interactions='normal.'),\n",
       " Row(interactions='normal.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rdd.select(\"interactions\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(interactions='normal.', duration=0),\n",
       " Row(interactions='normal.', duration=0),\n",
       " Row(interactions='normal.', duration=0),\n",
       " Row(interactions='normal.', duration=0),\n",
       " Row(interactions='normal.', duration=0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rdd.select(\"interactions\",\"duration\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`groupBy` works like pandas' `groupby`.\n",
    "\n",
    "The example below enables to count number of interactions according to protocol types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|protocol_type| count|\n",
      "+-------------+------+\n",
      "|          tcp|190065|\n",
      "|          udp| 20354|\n",
      "|         icmp|283602|\n",
      "+-------------+------+\n",
      "\n",
      "Times to execute the request : 5.895\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "df_rdd.groupBy(\"protocol_type\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Times to execute the request : {}\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`filter` a  *DataFrame* \n",
    "\n",
    "We count the interactions, according to protocol types , for interaction that last less than 1 second and without data transferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|protocol_type|count|\n",
      "+-------------+-----+\n",
      "|          tcp|  139|\n",
      "+-------------+-----+\n",
      "\n",
      "Requete executee en 6.154 secondes\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "df_rdd.filter(df_rdd.duration>1000).filter(df_rdd.dst_bytes==0).groupBy(\"protocol_type\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Requete executee en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the 'DataFrame' syntax allows much more flexible syntax that RDD's one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `map` and *custom function*\n",
    "\n",
    "The  `map` function is not available on *DataFrame* object. \n",
    "You will have to convert the  `DataFrame` to `RDD` to use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Duration: 0, Dest. bytes: 5450',\n",
       " 'Duration: 0, Dest. bytes: 486',\n",
       " 'Duration: 0, Dest. bytes: 1337',\n",
       " 'Duration: 0, Dest. bytes: 1337',\n",
       " 'Duration: 0, Dest. bytes: 2032']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rdd.rdd.map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes)).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to use the `udf` (*user defined function*).\n",
    "\n",
    "You first define the function you want to apply on each row, and convert it as an udf object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x, y)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "function = udf(lambda x,y: \"Duration: {}, Dest. bytes: {}\".format(x,y))\n",
    "function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can the apply this function to the wanted columns of the *DataFrame*. The `alias` function allow you to name the output columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(string_output='Duration: 0, Dest. bytes: 5450'),\n",
       " Row(string_output='Duration: 0, Dest. bytes: 486'),\n",
       " Row(string_output='Duration: 0, Dest. bytes: 1337'),\n",
       " Row(string_output='Duration: 0, Dest. bytes: 1337'),\n",
       " Row(string_output='Duration: 0, Dest. bytes: 2032')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dataframe = df_rdd.select(function(\"duration\",\"dst_bytes\").alias(\"string_output\"))\n",
    "output_dataframe.select(\"string_output\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column creation\n",
    "\n",
    "\n",
    "We have seen how to apply `udf` function on a *DataFrame*. The result is a one column *DataFrame*. You can add it to an existing *DataFrame* with the `withColumn` function.\n",
    "\n",
    "We will add a `label` columns to the `df` *DataFrame* which will have two possible values: `attack` and `normal`. \n",
    "\n",
    "We first create the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_or_normal_func(s):\n",
    "    return \"normal\" if s == \"normal.\" else \"attack\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is converted in a *user defined function*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_or_normal = udf(attack_or_normal_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the `label` column from the `df` *DataFrame* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_label = df.withColumn(\"label\", attack_or_normal(df.interactions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can know check that the column has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration: long (nullable = true)\n",
      " |-- protocol_type: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- land: long (nullable = true)\n",
      " |-- wrong_fragment: long (nullable = true)\n",
      " |-- urgent: long (nullable = true)\n",
      " |-- hot: long (nullable = true)\n",
      " |-- num_failed_logins: long (nullable = true)\n",
      " |-- logged_in: long (nullable = true)\n",
      " |-- num_compromised: long (nullable = true)\n",
      " |-- root_shell: long (nullable = true)\n",
      " |-- su_attempted: long (nullable = true)\n",
      " |-- num_root: long (nullable = true)\n",
      " |-- num_file_creations: long (nullable = true)\n",
      " |-- num_shells: long (nullable = true)\n",
      " |-- num_access_files: long (nullable = true)\n",
      " |-- num_outbound_cmds: long (nullable = true)\n",
      " |-- is_host_login: long (nullable = true)\n",
      " |-- is_guest_login: long (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- srv_count: long (nullable = true)\n",
      " |-- serror_rate: double (nullable = true)\n",
      " |-- srv_serror_rate: double (nullable = true)\n",
      " |-- rerror_rate: double (nullable = true)\n",
      " |-- srv_rerror_rate: double (nullable = true)\n",
      " |-- same_srv_rate: double (nullable = true)\n",
      " |-- diff_srv_rate: double (nullable = true)\n",
      " |-- srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_count: long (nullable = true)\n",
      " |-- dst_host_srv_count: long (nullable = true)\n",
      " |-- dst_host_same_srv_rate: double (nullable = true)\n",
      " |-- dst_host_diff_srv_rate: double (nullable = true)\n",
      " |-- dst_host_same_src_port_rate: double (nullable = true)\n",
      " |-- dst_host_srv_diff_host_rate: double (nullable = true)\n",
      " |-- dst_host_serror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_serror_rate: double (nullable = true)\n",
      " |-- dst_host_rerror_rate: double (nullable = true)\n",
      " |-- dst_host_srv_rerror_rate: double (nullable = true)\n",
      " |-- interactions: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_label.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**  Display number of attack and normal interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| label| count|\n",
      "+------+------+\n",
      "|normal| 97278|\n",
      "|attack|396743|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_label.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise3_1.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**  Display number of attack and normal interaction for each protocol_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+\n",
      "|protocol_type| label| count|\n",
      "+-------------+------+------+\n",
      "|          tcp|normal| 76813|\n",
      "|          udp|normal| 19177|\n",
      "|         icmp|normal|  1288|\n",
      "|          udp|attack|  1177|\n",
      "|          tcp|attack|113252|\n",
      "|         icmp|attack|282314|\n",
      "+-------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_label.groupBy(\"protocol_type\", \"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise3_2.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check all the functionality on the online [documentation](http://spark.apache.org/docs/latest/sql-programming-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas `udf` function\n",
    "\n",
    "A `pandas udf` function is similar to `udf` function previously defined. As the latest, it allow to apply transformation to a *DataFrame* column but this columns will be treated as a *pandas*' *series*.\n",
    "\n",
    "This will allow better [performances](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html) and also to apply native function of *pandas*. \n",
    "\n",
    "They are two types of `pandas udf`: `Scalar` and `Grouped Map`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Scalar`\n",
    "\n",
    "*Scalar Pandas UDFs* are used to easily vectorized scalar operation. It will handle p*andas* *Series* as an argument and return *Series* de *pandas* of same size.\n",
    "\n",
    "Here we will apply *pandas* `cumsum` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def cum_sum(x):\n",
    "    return x.cumsum()\n",
    "\n",
    "cum_sum_udf = pandas_udf(cum_sum, returnType=IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_sum_duration = df.select(cum_sum_udf(col(\"duration\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction *Grouped Map*\n",
    "\n",
    " *Grouped map Pandas UDFs* need to be used along with `groupBy().apply()` function allowing to apply `split-apply-combine` pattern. \n",
    " This one is a three steps job:\n",
    " \n",
    "\n",
    " * *Split* data with `DataFrame.groupBy`.\n",
    " * *Apply* function on each group. Input and output will be *pandas*' *DataFrames* output. L\n",
    " * *Combine* results in a new *DataFrame*.\n",
    "\n",
    "Before using `groupBy().apply()`, you have to define\n",
    "\n",
    "* A python function which will be applied on each group\n",
    "* A `StructType`  or a  `string` which will specify the schema of the output.\n",
    "\n",
    "In the following example we will subtract mean `duration` according to the label `attack` or `normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"label string, duration int\", PandasUDFType.GROUPED_MAP)\n",
    "def substract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    duration = pdf.duration\n",
    "    return pdf.assign(duration=duration - duration.mean())\n",
    "\n",
    "df_with_label.select(\"label\",\"duration\").groupby(\"label\").apply(substract_mean).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
