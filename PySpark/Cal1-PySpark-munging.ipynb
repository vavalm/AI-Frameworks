{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IA Framework.\n",
    "## Lab 1  - Introduction to Pyspark.\n",
    "#### Part 1 Data munging with <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resume**: The objective of this notebook is to discover [Spark](https://spark.apache.org/) framework and its python API [`PySpark`](http://spark.apache.org/docs/latest/api/python/). \n",
    "We will see the main motivation to use these frameworks that allow to apply distributed task on clusters and understand the concept of **RDD**(*Resilient Distributed Datasets*), main abstraction Spark provides , and how we can use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### When using Spark ?\n",
    "\n",
    "When data becomes too big for either RAM or Disk memory and/or where computation time are too high for your computer.  \n",
    "**Spark** allow you do parallelise your tasks to different clusters and provides an interface to do it easily. \n",
    "\n",
    "Machine learning algorithm (supervised or unsupervised) are iterative algorithm. Using them with *Hadoop* technology requires to read and write at each iteration on disk which will make learning really slow. [Spark](http://spark.apache.org/)' (*Resilient Distributed Dataset* or **RDD** [Zaharia et al. 2012](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf). We won't get deeply into Hadoop details. See [Besse et al. 2016](https://hal.archives-ouvertes.fr/hal-01350099v2/document) for an introduction to *MapReduce*, its limitation. \n",
    "\n",
    "Spark allow to stock data in each cluster trough **RDD** and use them only in read-only mode which allow faster run of different algorithms.\n",
    "\n",
    "\n",
    "#### About spark\n",
    "\n",
    "* **Spark** can be used trough *Java*, *Scala*, *R* and *Python* API. We will see  [`PySpark`](http://spark.apache.org/docs/latest/api/python/) API all along this TP. \n",
    "* **Spark** has four main librairies:\n",
    " \n",
    "    * [`SparlSQL`](https://spark.apache.org/docs/latest/sql-programming-guide.html) which allows to access really big and various kind of data, structured or not by executing SQL syntax. (Part 3).\n",
    "    * [`MLlib`](http://spark.apache.org/docs/latest/ml-guide.html) which contains statistical and machine learning algorithm (Part 2 and 4).\n",
    "    * [`SparkStreaming`](https://spark.apache.org/docs/latest/streaming-programming-guide.html) for live data stream processing (No cover in this TP).\n",
    "    * [`GraphX`](https://spark.apache.org/docs/latest/graphx-programming-guide.html) for graphs and graph-parallel computation (No cover in this TP).\n",
    "\n",
    "\n",
    "\n",
    "#### Spark locally\n",
    "**Spark** is designed to run on cluster to take advantage of it.\n",
    "In this TP we will run Spark locally to understand how to use *Pyspark* API, but we won't be able to realise the all potential of Spark.\n",
    "\n",
    "\n",
    "#### Warnings\n",
    "\n",
    "For the training phase of a machine learning model estimation, it's often easier to use bigger ressources (more Ram and CPU) than using **Spark** on distributed cluster. Using **Spark** (or **Hadoop**) will be more efficient for extracting sampling, preprocessing the data (See *Cdiscount* application in [Besse et al. 2016](https://hal.archives-ouvertes.fr/hal-01350099v2/document). \n",
    "\n",
    "\n",
    "#### References\n",
    "Official [documentation](https://spark.apache.org/docs/latest/). [Karau et al. (2015)](http://index-of.co.uk/Big-Data-Technologies/Learning%20Spark%20%20Lightning-Fast%20Big%20Data%20Analysis%20.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "**Spark** needs a configuration to be used. We create for that a `SparkConf` object that contains information about your application (where is the master, the cluster, etc.) and  `SparkContext` object which tell us how to access a cluster.\n",
    "\n",
    "Here, these objects have already been defined within the pyspark kernel of your notebook, and can be directly used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "This notebook has been inspired by the one build by [J. A. Dianes](https://github.com/jadianes/spark-py-notebooks) which use the dataset used in the  [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) competition. It contains 9M of interactions within a network (see description [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names)). The objective of this contest was to detect attack in a network from various features build or computed on transaction or interaction with this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# Download the file from URL\n",
    "DATA_PATH=\"\"  #<-- Put here where you want to stock the file.\n",
    "f = urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\",DATA_PATH+\"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From  a python object\n",
    "You can convert a python object to a RDD with the `parallelize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = range(100)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize(l)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a file\n",
    "The other way is to read a file from your computer as a RDD with the `textfile` function. We will now read the KDD dataset previously described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = DATA_PATH+\"kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "raw_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformations** and **Actions** on RDD.\n",
    "\n",
    "They are two types of operations you can apply on RDD **Transformations** and **Actions**. \n",
    "\n",
    "* A **Transformation** allows to create a new RDD from an existing one.\n",
    "* An **Action** applies computation on the RDD and return a value (like the `.count` function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.map` function is one of the simplest **transformation**. It allows to apply the same function to each entries of the RDD.\n",
    "Below we apply a function to split each string entry of the RDD to get a list of elements :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time execution :0.0008 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "ts = time.time()\n",
    "csv_data = raw_data.map(lambda x : x.split(\",\"))\n",
    "te= time.time()\n",
    "print(\"Time execution :%.4f seconds\" %(te-ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now observe the new RDD produced by the transformation : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take\n",
    "The `.take` function is one of the simplest **action**. It convert the RDD to a python list of n element(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', 'tcp', 'http', 'SF', '181', '5450', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '8', '8', '0.00', '0.00', '0.00', '0.00', '1.00', '0.00', '0.00', '9', '9', '1.00', '0.00', '0.11', '0.00', '0.00', '0.00', '0.00', '0.00', 'normal.'], ['0', 'tcp', 'http', 'SF', '239', '486', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '8', '8', '0.00', '0.00', '0.00', '0.00', '1.00', '0.00', '0.00', '19', '19', '1.00', '0.00', '0.05', '0.00', '0.00', '0.00', '0.00', '0.00', 'normal.']]\n",
      "Time execution :0.1316 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "ts = time.time()\n",
    "csv_take  = csv_data.take(2)\n",
    "print(csv_take)\n",
    "te =time.time()\n",
    "print(\"Time execution :%.4f seconds\" %(te-ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** \n",
    "* What can you say about the execution’s time of the two cells above? Is it normal? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de l'execution du .map(), l'opération ne s'était pas encore effectuée au niveau du données. Elle ne s'est effectuée que lorsque l'on a fait appelle à la méthode take(). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A more complex map function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first example of the `.map` function in the cell above, the function applied on each entry has been first defined within the `lambda` operator. The `lambda` operator is a one-line statement function. It doesn’t allow you to define complex function.\n",
    "\n",
    "\n",
    "You can also define first a python function and then apply it on the RDD. This will allow you to define more complex and more readable function.\n",
    "\n",
    "Below is a function that convert element to float, if the string can be converted to a float, and separate features from the label for each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([0.0, 'tcp', 'http', 'SF', 181.0, 5450.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 8.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 9.0, 9.0, 1.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0], 'normal.')]\n"
     ]
    }
   ],
   "source": [
    "# Fontion qui sépare les champ (elems=valeur) et extrait la 41ème = clef.\n",
    "def parse_interaction(l):\n",
    "    elems = l\n",
    "    features =[]\n",
    "    for e in  elems[:-1]:\n",
    "        try:\n",
    "            e=float(e)\n",
    "        except ValueError:\n",
    "            e=e\n",
    "        features.append(e)\n",
    "    y = elems[-1]\n",
    "    return (features, y)\n",
    "# Affichage des 5 premiers\n",
    "key_csv_data = csv_data.map(parse_interaction)\n",
    "print(key_csv_data.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `count` function is an **action** which allows to count number of elements that composes a RDD. \n",
    "\n",
    "Let's compute the number of element in the three RDDs created so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494021\n",
      "Time execution :0.6208 seconds\n",
      "494021\n",
      "Time execution :1.1983 seconds\n",
      "494021\n",
      "Time execution :6.5328 seconds\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "print(raw_data.count())\n",
    "te= time.time()\n",
    "print(\"Time execution :%.4f seconds\" %(te-ts))\n",
    "\n",
    "\n",
    "ts = time.time()\n",
    "print(csv_data.count())\n",
    "te= time.time()\n",
    "print(\"Time execution :%.4f seconds\" %(te-ts))\n",
    "\n",
    "ts = time.time()\n",
    "print(key_csv_data.count())\n",
    "te= time.time()\n",
    "print(\"Time execution :%.4f seconds\" %(te-ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** \n",
    "* What can you say about the results of the three applications of the count function?\n",
    "* What can you say about the execution’s times?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Réponses** \n",
    "- raw_data contient directement la sortie du fichier csv, il n'y a pas de transformations préalables à effectuer\n",
    "- csv_data quant à lui nécessite le prétraitement à effectuer (voir définition de la variable) avant de compter \n",
    "- key_csv_data : de même, il découle d'une pipeline plus longue qui doit s'exécuter de bout en bout avant le comptage final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinct\n",
    "\n",
    "The `distinct` function is a **transformation** that builds a RDD where all duplicated elements are removed.\n",
    "\n",
    "In the cell below we build a list of all different protocols within the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original :  ['tcp', 'tcp', 'tcp', 'tcp', 'tcp', 'tcp', 'tcp', 'tcp', 'tcp', 'tcp']\n",
      "après application de unique :  ['tcp', 'udp', 'icmp']\n"
     ]
    }
   ],
   "source": [
    "protocol = csv_data.map(lambda x : x[1]).distinct()\n",
    "print(\"original : \", csv_data.map(lambda x : x[1]).take(10))\n",
    "print(\"après application de unique : \", protocol.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter\n",
    "\n",
    "Another very used **transformation** is the `filter` function. It creates a smaller dataset based on a custom condition function.\n",
    "\n",
    "The cell below, will build a smaller RDD which contains only '.normal' entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_csv_data = csv_data.filter(lambda x: 'normal.' == x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are 97278 normal interactions (over 494021)\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "normal_count = normal_csv_data.count()\n",
    "total_count = csv_data.count()\n",
    "tt = time.time() - t0\n",
    "print(\"They are %d normal interactions (over %d)\" %(normal_count, total_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect\n",
    "`collect` is an **action** similar to `.take` **action**,  except that it will convert the entire dataset to a python list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected in 2.948 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "all_raw_data = raw_data.collect()\n",
    "tt = time.time() - t0\n",
    "print(\"Data collected in %.3f seconds\" %tt)\n",
    "all_raw_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected in 1.646 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use RDD is faster than python list\n",
    "t0 = time.time()\n",
    "all_raw_data = raw_data.take(raw_data.count())\n",
    "tt = time.time() - t0\n",
    "print(\"Data collected in %.3f seconds\" %tt)\n",
    "all_raw_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494021\n",
      "Time runing : 0.000\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "print(len(all_raw_data))\n",
    "tt = time.time() - t0\n",
    "print(\"Time runing : %.3f\" %tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining transformation\n",
    "\n",
    "You've seen above that **transformations** are *lazy* operators. Combining various **transformations** on various RDD are also *lazy*, i.e. the all pipeline of functions will be run only when a **action** is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time runing : 0.017\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "data_file = DATA_PATH+\"kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "normal_key_interactions = key_csv_data.filter(lambda x: x[0] == \"normal.\")\n",
    "tt = time.time() - t0\n",
    "print(\"Time runing : %.3f\" %tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time runing : 33.790\n",
      "There are 0 'normal' interactions\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "count_normal_interaction= normal_key_interactions.count()\n",
    "tt = time.time() - t0\n",
    "print(\"Time runing : %.3f\" %tt)\n",
    "print(\"There are %d 'normal' interactions\" %count_normal_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on rdd.\n",
    "Operations are **Transformation** that are applied on different RDDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtract\n",
    "The `Subtract` operation enables to create a RDD *C* from RDDs *A* and *B* by taking all entries from *A* that are not in *B*.\n",
    "\n",
    "The cells below allow to create a RDD with no normal data in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_raw_data = raw_data.filter(lambda x: \"normal.\" in x) #<- RDD with normal data\n",
    "attack_raw_data = raw_data.subtract(normal_raw_data) #<-- RDD with 'anormal' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All count in 0.512 secs\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "raw_data_count = raw_data.count()\n",
    "tt = time.time() - t0\n",
    "print(\"All count in {} secs\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count in 0.615 secs\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "normal_raw_data_count = normal_raw_data.count()\n",
    "tt = time.time() - t0\n",
    "print(\"Normal count in {} secs\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack count in 4.173 secs\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "attack_raw_data_count = attack_raw_data.count()\n",
    "tt = time.time() - t0\n",
    "print(\"Attack count in {} secs\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 97278 interactions normales et 396743 attaques, pour un total de 494021 interactions\n"
     ]
    }
   ],
   "source": [
    "print(\"Il y a {} interactions normales et {} attaques, pour un total de {} interactions\".format(normal_raw_data_count,attack_raw_data_count,raw_data_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cartesian\n",
    "\n",
    "The cartesian product (`.cartesian`) returns every possible pairs between elements of two RDDs. (To be used with caution if RDDs are really big).\n",
    "\n",
    "**Exercise**\n",
    "Write function do display all possible pair of existing protocols (second columns) and services (third column) within the dataset using  `.cartesian` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 198 combinaisons de protocol X service\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/exercise1_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protocols = csv_data.map(lambda x: x[1]).distinct()\n",
    "services = csv_data.map(lambda x: x[2]).distinct()\n",
    "\n",
    "\n",
    "protocols_services_combinaisons = protocol.cartesian(services)\n",
    "protocols_services_combinaisons.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More **Actions** on RDDs\n",
    "So far we have used only *native* action function (count, distinct, take etc..) We will now define custom action with `reduce` and `aggregate` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `reduce` function takes a function as an argument that describes how elements from the RDD are combined.\n",
    "\n",
    "The code below will build two RDDs containing duration time from normal and attack data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normal_csv_data =csv_data.filter(lambda x : x[-1]==\"normal.\")\n",
    "attack_csv_data =csv_data.filter(lambda x : x[-1]!=\"normal.\")\n",
    "\n",
    "normal_duration_data = normal_csv_data.map(lambda x: int(x[0]))\n",
    "attack_duration_data = attack_csv_data.map(lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We use then the `reduce` function to compute the total duration of all these actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration for 'normal' interactions is 21075991\n",
      "Total duration for 'attack' interactions is 2626792\n"
     ]
    }
   ],
   "source": [
    "total_normal_duration = normal_duration_data.reduce(lambda x, y: x + y)\n",
    "total_attack_duration = attack_duration_data.reduce(lambda x, y: x + y)\n",
    "\n",
    "print(\"Total duration for 'normal' interactions is {}\".\\\n",
    "    format(total_normal_duration))\n",
    "print(\"Total duration for 'attack' interactions is {}\".\\\n",
    "    format(total_attack_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We compute their means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean duration for 'normal' interactions is 216.657\n",
      "Mean duration for 'attack' interactions is 6.621\n"
     ]
    }
   ],
   "source": [
    "normal_count = normal_duration_data.count()\n",
    "attack_count = attack_duration_data.count()\n",
    "\n",
    "print(\"Mean duration for 'normal' interactions is {}\".\\\n",
    "    format(round(total_normal_duration/float(normal_count),3)))\n",
    "print(\"Mean duration for 'attack' interactions is {}\".\\\n",
    "    format(round(total_attack_duration/float(attack_count),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The code above looks quite complicated for such a simple operation. The reduce function does not allow to define a \"start\" value, like in Python. It implies that you have to first convert the RDD in the wanted format. \n",
    "Here, you have to first convert the string information about the attack within integer before to count it. \n",
    "\n",
    "\n",
    "**Question** : How would you implement the `count` function with `reduce` function in Python?  Why those function wouldn’t work if it was applied with Spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/exercise1_2.py\n",
    "from functools import reduce\n",
    "a = [x for x in range(100)]\n",
    "reduce(lambda a,b : a+1,a,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "#### aggregate\n",
    "\n",
    "The `aggregate` function allows to overcome the problem. The function takes as arguements : \n",
    "1. The initialization\n",
    "2. A function that describes how elements of the rdd are combined\n",
    "3. A function that describes how elements from different cluster will be combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21075991"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_sum_duration = normal_duration_data.aggregate(0, # valeurs initiales à 0\n",
    "    lambda acc, value: acc + value, # Somme des durées et cumul des interactions\n",
    "    lambda acc1, acc2: acc1+ acc2 # cumul des accumualteurs\n",
    "    )\n",
    "normal_sum_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The aggregate function could be used to compute both total sum of duration and count of element in one call of the function.\n",
    "\n",
    "**Exercise**: write a function that return total duraction of normal attack AND the count of normal attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durée moyenne des interactions agressives 6.621\n"
     ]
    }
   ],
   "source": [
    "attack_sum_count = attack_duration_data.aggregate(\n",
    "    (0,0), # the initial value\n",
    "    lambda value_count, new_value: (new_value + value_count[0], value_count[1] + 1),\n",
    "    lambda value_count1, value_count2: (value_count1[0] + value_count2[0], value_count1[1] + value_count2[1])\n",
    "    )\n",
    "\n",
    "print(\"Durée moyenne des interactions agressives {}\".\\\n",
    "    format(round(attack_sum_count[0]/float(attack_sum_count[1]),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durée moyenne des interactions agressives 6.621\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/exercise1_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReducebyKey & CombineByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`ReducebyKey` & `CombineByKey` are functions that allow to apply reduce or aggregate function by key (which is the base of MapReduce function).\n",
    "\n",
    "We need for that, to define a RDD where for each entry, the first element is the key, and the second element is the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.', 0.0),\n",
       " ('normal.', 0.0),\n",
       " ('normal.', 0.0),\n",
       " ('normal.', 0.0),\n",
       " ('normal.', 0.0),\n",
       " ('normal.', 0.0),\n",
       " ('normal.', 0.0),\n",
       " ('normal.', 0.0),\n",
       " ('normal.', 0.0),\n",
       " ('normal.', 0.0)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_value_duration = csv_data.map(lambda x: (x[41], float(x[0]))) # x[41] contient le type normal ou non\n",
    "key_value_duration.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Reduce By Key\n",
    "\n",
    "\n",
    "The `reduceByKey` function is used to apply reduce function for each key on the first columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.', 21075991.0),\n",
       " ('buffer_overflow.', 2751.0),\n",
       " ('loadmodule.', 326.0),\n",
       " ('perl.', 124.0),\n",
       " ('neptune.', 0.0),\n",
       " ('smurf.', 0.0),\n",
       " ('guess_passwd.', 144.0),\n",
       " ('pod.', 0.0),\n",
       " ('teardrop.', 0.0),\n",
       " ('portsweep.', 1991911.0),\n",
       " ('ipsweep.', 43.0),\n",
       " ('land.', 0.0),\n",
       " ('ftp_write.', 259.0),\n",
       " ('back.', 284.0),\n",
       " ('imap.', 72.0),\n",
       " ('satan.', 64.0),\n",
       " ('phf.', 18.0),\n",
       " ('nmap.', 0.0),\n",
       " ('multihop.', 1288.0),\n",
       " ('warezmaster.', 301.0),\n",
       " ('warezclient.', 627563.0),\n",
       " ('spy.', 636.0),\n",
       " ('rootkit.', 1008.0)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "durations_by_key = key_value_duration.reduceByKey(lambda x, y: x + y)\n",
    "durations_by_key.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We note that `reduceByKey` is a **Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'normal.': 97278,\n",
       "             'buffer_overflow.': 30,\n",
       "             'loadmodule.': 9,\n",
       "             'perl.': 3,\n",
       "             'neptune.': 107201,\n",
       "             'smurf.': 280790,\n",
       "             'guess_passwd.': 53,\n",
       "             'pod.': 264,\n",
       "             'teardrop.': 979,\n",
       "             'portsweep.': 1040,\n",
       "             'ipsweep.': 1247,\n",
       "             'land.': 21,\n",
       "             'ftp_write.': 8,\n",
       "             'back.': 2203,\n",
       "             'imap.': 12,\n",
       "             'satan.': 1589,\n",
       "             'phf.': 4,\n",
       "             'nmap.': 231,\n",
       "             'multihop.': 7,\n",
       "             'warezmaster.': 20,\n",
       "             'warezclient.': 1020,\n",
       "             'spy.': 2,\n",
       "             'rootkit.': 10})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_by_key = key_value_duration.countByKey()\n",
    "counts_by_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### combineByKey\n",
    "\n",
    "`combineByKey` if for `aggregate` whate `reduceByKey` is to `reduce` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.', (21075991.0, 97278)),\n",
       " ('buffer_overflow.', (2751.0, 30)),\n",
       " ('loadmodule.', (326.0, 9)),\n",
       " ('perl.', (124.0, 3)),\n",
       " ('neptune.', (0.0, 107201)),\n",
       " ('smurf.', (0.0, 280790)),\n",
       " ('guess_passwd.', (144.0, 53)),\n",
       " ('pod.', (0.0, 264)),\n",
       " ('teardrop.', (0.0, 979)),\n",
       " ('portsweep.', (1991911.0, 1040)),\n",
       " ('ipsweep.', (43.0, 1247)),\n",
       " ('land.', (0.0, 21)),\n",
       " ('ftp_write.', (259.0, 8)),\n",
       " ('back.', (284.0, 2203)),\n",
       " ('imap.', (72.0, 12)),\n",
       " ('satan.', (64.0, 1589)),\n",
       " ('phf.', (18.0, 4)),\n",
       " ('nmap.', (0.0, 231)),\n",
       " ('multihop.', (1288.0, 7)),\n",
       " ('warezmaster.', (301.0, 20)),\n",
       " ('warezclient.', (627563.0, 1020)),\n",
       " ('spy.', (636.0, 2)),\n",
       " ('rootkit.', (1008.0, 10))]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_counts = key_value_duration.combineByKey(\n",
    "    (lambda x: (x, 1)), # valeur initiale x and compteur 1\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # Combiner une paire  avec une paires d'accumulateurs (somme et incrément)\n",
    "    (lambda acc1, acc2: (acc1[0]+acxc2[0], acc1[1]+acc2[1])) # combinaison des accumulateurs\n",
    "     )\n",
    "sum_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "duration_means_by_type = sum_counts.map(lambda lambda_args: (lambda_args[0], round(lambda_args[1][0]/lambda_args[1][1],3)))\n",
    "duration_means_by_type.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.', 216.657),\n",
       " ('buffer_overflow.', 91.7),\n",
       " ('loadmodule.', 36.222),\n",
       " ('perl.', 41.333),\n",
       " ('neptune.', 0.0),\n",
       " ('smurf.', 0.0),\n",
       " ('guess_passwd.', 2.717),\n",
       " ('pod.', 0.0),\n",
       " ('teardrop.', 0.0),\n",
       " ('portsweep.', 1915.299),\n",
       " ('ipsweep.', 0.034),\n",
       " ('land.', 0.0),\n",
       " ('ftp_write.', 32.375),\n",
       " ('back.', 0.129),\n",
       " ('imap.', 6.0),\n",
       " ('satan.', 0.04),\n",
       " ('phf.', 4.5),\n",
       " ('nmap.', 0.0),\n",
       " ('multihop.', 184.0),\n",
       " ('warezmaster.', 15.05),\n",
       " ('warezclient.', 615.258),\n",
       " ('spy.', 318.0),\n",
       " ('rootkit.', 100.8)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "492px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}